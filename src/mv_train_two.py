from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
import random
import pprint
import sys
import traceback, logging
logging.basicConfig(level=logging.ERROR)
import time
import numpy as np
from optparse import OptionParser
import pickle
import math
import cv2
import copy
import threading
from matplotlib import pyplot as plt
import tensorflow as tf
import pandas as pd
import os
from utils import get_file_list_from_dir
from itertools import permutations
import datetime

from sklearn.metrics import average_precision_score

from keras import backend as K
from keras.optimizers import Adam, SGD, RMSprop
from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout, Concatenate, Reshape, Lambda
from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed
from keras.engine.topology import get_source_inputs
from keras.utils import layer_utils
from keras.utils.data_utils import get_file
from keras.objectives import categorical_crossentropy

from keras.models import Model, Sequential
from keras.utils import generic_utils
from keras.engine import Layer, InputSpec
from keras import initializers, regularizers
from keras.utils import multi_gpu_model
from keras.callbacks import TensorBoard

from tensorflow.keras.utils import plot_model

#os.environ['CUDA_VISIBLE_DEVICES']='0,1,3'
class Config:

    def __init__(self):

        # Print the process or not
        self.verbose = True

        # Name of base network
        self.network = 'vgg'

        # Setting for data augmentation
        self.use_horizontal_flips = False
        self.use_vertical_flips = False
        self.rot_90 = False

        # Anchor box scales
        # Note that if im_size is smaller, anchor_box_scales should be scaled
        # Original anchor_box_scales in the paper is [128, 256, 512]
        self.anchor_box_scales = [64, 128, 256] 
        # Anchor box ratios
        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]

        # Size to resize the smallest side of the image
        # Original setting in paper is 600. Set to 300 in here to save training time
        #self.im_size = 300
        self.im_size = 600

        self.num_features = 512
        

        # image channel-wise mean to subtract
        self.img_channel_mean = [103.939, 116.779, 123.68]
        self.img_scaling_factor = 1.0

        # number of ROIs at once
        self.num_rois = 4

        # stride at the RPN (this depends on the network configuration)
        self.rpn_stride = 16

        self.balanced_classes = False

        # scaling the stdev
        self.std_scaling = 4.0
        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]

        # overlaps for RPN
        self.rpn_min_overlap = 0.3
        self.rpn_max_overlap = 0.7

        # overlaps for classifier ROIs
        self.classifier_min_overlap = 0.1
        self.classifier_max_overlap = 0.5

        # placeholder for the class mapping, automatically generated by the parser
        self.class_mapping = None

        self.model_path = None

        self.raw_img_rows = 360
        self.raw_img_cols = 640
        self.F = 0
        self.epipolar_x_interval = 3

        self.num_cam = 3
        self.num_nms = 300
    
        self.vi_max_overlap = .3
        self.max_match_dist = np.inf
        self.vi_alpha = .3
        self.view_invar_feature_size = 128

        self.resize_img_rows= self.im_size #600
        self.resize_ratio = self.resize_img_rows / self.raw_img_rows
        self.resize_img_cols= self.raw_img_cols * self.resize_ratio #1066
        self.anchors = len(self.anchor_box_scales) * len(self.anchor_box_ratios) #9
        self.grid_rows = self.resize_img_rows//self.rpn_stride
        self.grid_cols = self.resize_img_cols//self.rpn_stride
        self.F_to_grid_ratio = self.raw_img_rows / self.grid_rows #9.6969
        self.whole_anchors = self.grid_rows * self.grid_cols * self.anchors
 
def get_concat_img(img_list, cols=3):
    rows = int(len(img_list)/cols)
    hor_imgs = [np.hstack(img_list[i*cols:(i+1)*cols]) for i in range(rows)]
    ver_imgs = np.vstack(hor_imgs)
    return ver_imgs


def format_img_size(img, C):
    """ formats the image size based on config """
    img_min_side = float(C.im_size)
    (height,width,_) = img.shape
        
    if width <= height:
        ratio = img_min_side/width
        new_height = int(ratio * height)
        new_width = int(img_min_side)
    else:
        ratio = img_min_side/height
        new_width = int(ratio * width)
        new_height = int(img_min_side)
    fx = width/float(new_width)
    fy = height/float(new_height)
    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    return img, ratio, fx, fy   

def format_img_channels(img, C):
    """ formats the image channels based on config """
    img = img[:, :, (2, 1, 0)]
    img = img.astype(np.float32)
    img[:, :, 0] -= C.img_channel_mean[0]
    img[:, :, 1] -= C.img_channel_mean[1]
    img[:, :, 2] -= C.img_channel_mean[2]
    img /= C.img_scaling_factor
    img = np.transpose(img, (2, 0, 1))
    img = np.expand_dims(img, axis=0)
    return img

def format_img(img, C):
    """ formats an image for model prediction based on config """
    img, ratio, fx, fy = format_img_size(img, C)
    img = format_img_channels(img, C)
    return img, ratio, fx, fy

# Method to transform the coordinates of the bounding box to its original size
def get_real_coordinates(ratio, x1, y1, x2, y2):

    real_x1 = int(round(x1 // ratio))
    real_y1 = int(round(y1 // ratio))
    real_x2 = int(round(x2 // ratio))
    real_y2 = int(round(y2 // ratio))

    return (real_x1, real_y1, real_x2 ,real_y2)

'''
def get_test_model(input_shape_img, input_shape_features, num_cam, num_rois, num_anchors, num_classes, model_path) :
    img_input = []
    roi_input = []
    feature_map_input = []
    for i in range(num_cam) : 
        img_input.append(Input(shape=input_shape_img))
        roi_input.append(Input(shape=(num_rois, 4)))
        feature_map_input.append(Input(shape=input_shape_features))

    # define the base network (VGG here, can be Resnet50, Inception, etc)
    shared_layer = nn_base_model()
    shared_layers = []
    for i in range(num_cam):
        shared_layers.append(shared_layer(img_input[i]))

    # define the RPN, built on the base layers
    rpn_body, rpn_class, rpn_regr = rpn_layer_model(num_anchors)
    rpns = []
    for i in range(num_cam) :
        body = rpn_body(shared_layers[i])
        cls = rpn_class(body)
        regr = rpn_regr(body)
        rpns.extend([cls, regr, shared_layers[i]])

    classifier = classifier_layer(feature_map_input, roi_input, num_rois, num_cam, nb_classes=num_classes)

    model_rpn = Model(img_input, rpns)
    classifier_input = feature_map_input + roi_input
    model_classifier = Model(classifier_input, classifier)

    print('Loading weights from {}'.format(model_path))
    model_rpn.load_weights(model_path, by_name=True)
    model_classifier.load_weights(model_path, by_name=True)

    model_rpn.compile(optimizer='sgd', loss='mse')
    model_classifier.compile(optimizer='sgd', loss='mse')

    return model_rpn, model_classifier
'''

def get_test_model(input_shape_img, input_shape_features, num_cam, num_rois, num_anchors, num_classes, model_path) :
    img_input = []
    roi_input = []
    feature_map_input = []
    rpn_body_input = []
    for i in range(num_cam) : 
        img_input.append(Input(shape=input_shape_img))
        roi_input.append(Input(shape=(num_rois, 4)))
        feature_map_input.append(Input(shape=input_shape_features))
        rpn_body_input.append(Input(shape=input_shape_features))

    # define the base network (VGG here, can be Resnet50, Inception, etc)
    shared_layer = nn_base_model()
    shared_layers = []
    for i in range(num_cam):
        shared_layers.append(shared_layer(img_input[i]))

    # define the RPN, built on the base layers
    rpn_body, rpn_class, rpn_regr = rpn_layer_model(num_anchors)
    view_invariant_layer = view_invariant_layer_model(C.grid_rows, int(C.grid_cols), num_anchors, C.view_invar_feature_size)
    rpns = []
    view_invariants = []
    for i in range(num_cam) :
        body = rpn_body(shared_layers[i])
        cls = rpn_class(body)
        regr = rpn_regr(body)
        rpns.extend([cls, regr, shared_layers[i], body])
        view_invariant = view_invariant_layer(rpn_body_input[i])
        view_invariants.append(view_invariant)

    view_invariant_conc = view_invariant_conc_layer(view_invariants)
    classifier = classifier_layer(feature_map_input, roi_input, num_rois, C.num_features, num_cam, nb_classes=num_classes)

    model_rpn = Model(img_input, rpns)
    model_view_invariant = Model(rpn_body_input, view_invariant_conc)
    classifier_input = feature_map_input + roi_input
    model_classifier = Model(classifier_input, classifier)

    print('Loading weights from {}'.format(model_path))
    model_rpn.load_weights(model_path, by_name=True)
    model_view_invariant.load_weights(model_path, by_name=True)
    model_classifier.load_weights(model_path, by_name=True)

    model_rpn.compile(optimizer='sgd', loss='mse')
    model_view_invariant.compile(optimizer='sgd', loss='mse')
    model_classifier.compile(optimizer='sgd', loss='mse')

    return model_rpn, model_view_invariant, model_classifier

def get_class_mapping(C):
    class_mapping = C.class_mapping
    class_mapping = {v: k for k, v in class_mapping.items()}
    #print(class_mapping)
    return class_mapping

def get_img_list(img_data, C):
    img_list = []
    for i in range(C.num_cam) :
        filepath = img_data[i]['filepath']
        img = cv2.imread(filepath)
        img_list.append(img)
    return img_list

def get_X(img, C) : 
    X, ratio, fx, fy = format_img(img, C)
    X = np.transpose(X, (0, 2, 3, 1))
    return X, ratio, fx, fy

def get_X_list(img_list, C):
    X_list = []
    for img in img_list :
        X, ratio, fx, fy = get_X(img, C)
        X_list.append(X)
    return X_list, ratio, fx, fy

def draw_cls_box_prob(img_list, bboxes, probs, C, is_nms=True) : 
    height,_,_ = img_list[0].shape
    img_min_side = float(C.im_size)
    ratio = img_min_side/height

    all_dets = []
    for key in bboxes:
        bbox = np.array(bboxes[key]) #(num_cam, num_box, 4)
        if(is_nms):
            new_boxes_all_cam, new_probs = non_max_suppression_fast_multi_cam(bbox, np.array(probs[key]), overlap_thresh=0.5)
        else : 
            new_boxes_all_cam, new_probs = bbox, np.array(probs[key])
        instance_to_color = [np.random.randint(0, 255, 3) for _ in range(len(new_probs))]
        for cam_idx in range(C.num_cam) : 
            img = img_list[cam_idx]
            new_boxes = new_boxes_all_cam[cam_idx] #(num_box, 4)
            for jk in range(new_boxes.shape[0]):
                (x1, y1, x2, y2) = new_boxes[jk,:]
                if(x1 == -C.rpn_stride) :
                    continue 
                # Calculate real coordinates on original image
                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)

                color = (int(instance_to_color[jk][0]), int(instance_to_color[jk][1]), int(instance_to_color[jk][2])) 
                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), color, 4)
                textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))
                all_dets.append((key,100*new_probs[jk]))
                (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)
                textOrg = (real_x1, real_y1-0)
                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)
                cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)
                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)
    print(all_dets)
    plt.figure(figsize=(10,10))
    for i, img in enumerate(img_list) : 
        coord = int('1'+str(C.num_cam)+str(i+1))
        plt.subplot(coord)
        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))
    plt.show()

def classfier_output_to_box_prob(ROIs_list, P_cls, P_regr, C, bbox_threshold, is_demo) : 
    class_mapping = get_class_mapping(C)
    bboxes = {}
    probs = {}
    # Calculate bboxes coordinates on resized image
    for ii in range(P_cls.shape[1]):
        '''
        # Ignore 'bg' class
        if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1) : 
            continue
        '''

        if np.max(P_cls[0, ii, :]) < bbox_threshold and is_demo :
            continue

        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]

        if cls_name not in bboxes:
            bboxes[cls_name] = [[] for _ in range(C.num_cam)]
            probs[cls_name] = []

        cls_num = np.argmax(P_cls[0, ii, :])
        
        cam_offset = (len(C.class_mapping) - 1) * 4
        for cam_idx in range(C.num_cam) : 
            #(x, y, w, h) = ROIs[0, ii, :]
            (x, y, w, h) = ROIs_list[cam_idx][0, ii, :]
            try:
                #(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]
                (tx, ty, tw, th) = P_regr[0, ii, cam_offset*cam_idx + 4*cls_num : cam_offset*cam_idx + 4*(cls_num+1)]
                tx /= C.classifier_regr_std[0]
                ty /= C.classifier_regr_std[1]
                tw /= C.classifier_regr_std[2]
                th /= C.classifier_regr_std[3]
                x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)
            except:
                pass
            bboxes[cls_name][cam_idx].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])
        probs[cls_name].append(np.max(P_cls[0, ii, :]))
    return bboxes, probs 

'''
def get_bboxes_probs(X_list, model_rpn, model_classifier, bbox_threshold, class_mapping, C, is_demo=1):
    # get output layer Y1, Y2 from the RPN and the feature maps F
    P_rpn = model_rpn.predict(X_list)

    # Get bboxes by applying NMS 
    # R.shape = (300, 4)
    R_list = []
    for i in range(C.num_cam):
        cam_idx = i*3
        rpn_probs = P_rpn[cam_idx]
        rpn_boxs = P_rpn[cam_idx+1]
        R = rpn_to_roi(rpn_probs, rpn_boxs, C, K.common.image_dim_ordering(), overlap_thresh=0.7)
        R_list.append(R)
       
    grouped_R = epipolar(R_list, C, None, 0)

    # convert from (x1,y1,x2,y2) to (x,y,w,h)
    for i in range(C.num_cam) : 
        grouped_R[i][:, 2] -= grouped_R[i][:, 0] 
        grouped_R[i][:, 3] -= grouped_R[i][:, 1] 

    # apply the spatial pyramid pooling to the proposed regions
    bboxes = {}
    probs = {}

    for jk in range(grouped_R[0].shape[0]//C.num_rois + 1):
        ROIs_list = []
        F_list = []
        for cam_idx in range(C.num_cam):
            R = grouped_R[cam_idx]
            ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)
            if ROIs.shape[1] == 0:
                break

            if jk == grouped_R[0].shape[0]//C.num_rois:
                #pad R
                curr_shape = ROIs.shape
                target_shape = (curr_shape[0],C.num_rois,curr_shape[2])
                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)
                ROIs_padded[:, :curr_shape[1], :] = ROIs
                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]
                ROIs = ROIs_padded
            ROIs_list.append(ROIs)
            F = P_rpn[cam_idx*3 + 2]
            F_list.append(F)

        if len(ROIs_list) == 0:
            break

        [P_cls, P_regr] = model_classifier.predict_on_batch(F_list + ROIs_list)

        # Calculate bboxes coordinates on resized image
        for ii in range(P_cls.shape[1]):
            # Ignore 'bg' class
            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1) : 
                continue

            if np.max(P_cls[0, ii, :]) < bbox_threshold and is_demo :
                continue

            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]

            if cls_name not in bboxes:
                #bboxes[cls_name] = []
                bboxes[cls_name] = [[] for _ in range(C.num_cam)]
                probs[cls_name] = []

            cls_num = np.argmax(P_cls[0, ii, :])
            
            cam_offset = (len(class_mapping) - 1) * 4
            for cam_idx in range(C.num_cam) : 
                #(x, y, w, h) = ROIs[0, ii, :]
                (x, y, w, h) = ROIs_list[cam_idx][0, ii, :]
                try:
                    #(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]
                    (tx, ty, tw, th) = P_regr[0, ii, cam_offset*cam_idx + 4*cls_num : cam_offset*cam_idx + 4*(cls_num+1)]
                    tx /= C.classifier_regr_std[0]
                    ty /= C.classifier_regr_std[1]
                    tw /= C.classifier_regr_std[2]
                    th /= C.classifier_regr_std[3]
                    x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)
                except:
                    pass
                bboxes[cls_name][cam_idx].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])
            probs[cls_name].append(np.max(P_cls[0, ii, :]))
    return bboxes, probs 
'''

def get_bboxes_probs_from_train_model(X_list, model_rpn, model_view_invariant, model_classifier, bbox_threshold, class_mapping, C, is_demo=1):
    # get output layer Y1, Y2 from the RPN and the feature maps F
    P_rpn = model_rpn.predict(X_list)

    # Get bboxes by applying NMS 
    # R.shape = (300, 4)
    R_list = []
    for i in range(C.num_cam):
        cam_idx = i*2
        rpn_probs = P_rpn[cam_idx]
        rpn_boxs = P_rpn[cam_idx+1]
        R = rpn_to_roi(rpn_probs, rpn_boxs, C, K.image_data_format(), overlap_thresh=0.7)
        R_list.append(R)
       
    view_invariant_features = model_view_invariant.predict_on_batch(X_list)
    grouped_R, anchor_cam_idx = reid300(view_invariant_features, R_list, C)
    '''
    debug_img = np.squeeze(np.array(X_list))
    draw_reid(grouped_R, anchor_cam_idx, debug_img, C.rpn_stride)
    '''

    # convert from (x1,y1,x2,y2) to (x,y,w,h)
    for i in range(C.num_cam) : 
        grouped_R[i][:, 2] -= grouped_R[i][:, 0] 
        grouped_R[i][:, 3] -= grouped_R[i][:, 1] 

    # apply the spatial pyramid pooling to the proposed regions
    bboxes = {}
    probs = {}

    for jk in range(grouped_R[0].shape[0]//C.num_rois + 1):
        ROIs_list = []
        for cam_idx in range(C.num_cam):
            R = grouped_R[cam_idx]
            ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)
            if ROIs.shape[1] == 0:
                break

            if jk == grouped_R[0].shape[0]//C.num_rois:
                #pad R
                curr_shape = ROIs.shape
                target_shape = (curr_shape[0],C.num_rois,curr_shape[2])
                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)
                ROIs_padded[:, :curr_shape[1], :] = ROIs
                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]
                ROIs = ROIs_padded
            ROIs_list.append(ROIs)

        if len(ROIs_list) == 0:
            break

        [P_cls, P_regr] = model_classifier.predict_on_batch(X_list + ROIs_list)

        # Calculate bboxes coordinates on resized image
        for ii in range(P_cls.shape[1]):
            # Ignore 'bg' class
            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1) : 
                continue

            if np.max(P_cls[0, ii, :]) < bbox_threshold and is_demo :
                continue

            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]

            if cls_name not in bboxes:
                #bboxes[cls_name] = []
                bboxes[cls_name] = [[] for _ in range(C.num_cam)]
                probs[cls_name] = []

            cls_num = np.argmax(P_cls[0, ii, :])
            
            cam_offset = (len(class_mapping) - 1) * 4
            for cam_idx in range(C.num_cam) : 
                #(x, y, w, h) = ROIs[0, ii, :]
                (x, y, w, h) = ROIs_list[cam_idx][0, ii, :]
                try:
                    #(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]
                    (tx, ty, tw, th) = P_regr[0, ii, cam_offset*cam_idx + 4*cls_num : cam_offset*cam_idx + 4*(cls_num+1)]
                    tx /= C.classifier_regr_std[0]
                    ty /= C.classifier_regr_std[1]
                    tw /= C.classifier_regr_std[2]
                    th /= C.classifier_regr_std[3]
                    x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)
                except:
                    pass
                bboxes[cls_name][cam_idx].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])
            probs[cls_name].append(np.max(P_cls[0, ii, :]))
    return bboxes, probs 

def get_bboxes_probs(X_list, model_rpn, model_view_invariant, model_classifier, bbox_threshold, class_mapping, C, is_demo=1):
    # get output layer Y1, Y2 from the RPN and the feature maps F
    P_rpn = model_rpn.predict(X_list)

    # Get bboxes by applying NMS 
    # R.shape = (300, 4)
    R_list = []
    for i in range(C.num_cam):
        cam_idx = i*4
        rpn_probs = P_rpn[cam_idx]
        rpn_boxs = P_rpn[cam_idx+1]
        #R = rpn_to_roi(rpn_probs, rpn_boxs, C, K.common.image_dim_ordering(), overlap_thresh=0.7)
        R = rpn_to_roi(rpn_probs, rpn_boxs, C, K.image_data_format(), overlap_thresh=0.7)
        R_list.append(R)
       
    #grouped_R = epipolar(R_list, C, None, 0)
    rpn_body_list = [P_rpn[i*4+3] for i in range(C.num_cam)]
    view_invariant_features = model_view_invariant.predict_on_batch(rpn_body_list)
    #grouped_R = reid(view_invariant_features, R_list, C)
    grouped_R, anchor_cam_idx = reid300(view_invariant_features, R_list, C)
    '''
    debug_img = np.squeeze(np.array(X_list))
    draw_reid(grouped_R, anchor_cam_idx, debug_img, C.rpn_stride)
    '''

    # convert from (x1,y1,x2,y2) to (x,y,w,h)
    for i in range(C.num_cam) : 
        grouped_R[i][:, 2] -= grouped_R[i][:, 0] 
        grouped_R[i][:, 3] -= grouped_R[i][:, 1] 

    # apply the spatial pyramid pooling to the proposed regions
    bboxes = {}
    probs = {}

    for jk in range(grouped_R[0].shape[0]//C.num_rois + 1):
        ROIs_list = []
        F_list = []
        for cam_idx in range(C.num_cam):
            R = grouped_R[cam_idx]
            ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)
            if ROIs.shape[1] == 0:
                break

            if jk == grouped_R[0].shape[0]//C.num_rois:
                #pad R
                curr_shape = ROIs.shape
                target_shape = (curr_shape[0],C.num_rois,curr_shape[2])
                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)
                ROIs_padded[:, :curr_shape[1], :] = ROIs
                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]
                ROIs = ROIs_padded
            ROIs_list.append(ROIs)
            F = P_rpn[cam_idx*4 + 2]
            F_list.append(F)

        if len(ROIs_list) == 0:
            break

        [P_cls, P_regr] = model_classifier.predict_on_batch(F_list + ROIs_list)

        # Calculate bboxes coordinates on resized image
        for ii in range(P_cls.shape[1]):
            # Ignore 'bg' class
            if np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1) : 
                continue

            if np.max(P_cls[0, ii, :]) < bbox_threshold and is_demo :
                continue

            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]

            if cls_name not in bboxes:
                #bboxes[cls_name] = []
                bboxes[cls_name] = [[] for _ in range(C.num_cam)]
                probs[cls_name] = []

            cls_num = np.argmax(P_cls[0, ii, :])
            
            cam_offset = (len(class_mapping) - 1) * 4
            for cam_idx in range(C.num_cam) : 
                #(x, y, w, h) = ROIs[0, ii, :]
                (x, y, w, h) = ROIs_list[cam_idx][0, ii, :]
                try:
                    #(tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]
                    (tx, ty, tw, th) = P_regr[0, ii, cam_offset*cam_idx + 4*cls_num : cam_offset*cam_idx + 4*(cls_num+1)]
                    tx /= C.classifier_regr_std[0]
                    ty /= C.classifier_regr_std[1]
                    tw /= C.classifier_regr_std[2]
                    th /= C.classifier_regr_std[3]
                    x, y, w, h = apply_regr(x, y, w, h, tx, ty, tw, th)
                except:
                    pass
                bboxes[cls_name][cam_idx].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])
            probs[cls_name].append(np.max(P_cls[0, ii, :]))
    return bboxes, probs 

def demo(test_img_data, model_rpn, model_view_invariant, model_classifier, bbox_threshold, C, result_dir):
    class_mapping = get_class_mapping(C)
    class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}

    img_list = get_img_list(test_img_data, C)
    X_list, ratio, _, _ = get_X_list(img_list, C)
    bboxes, probs = get_bboxes_probs(X_list, model_rpn, model_view_invariant, model_classifier, bbox_threshold, class_mapping, C)

    all_dets = []
    for key in bboxes:
        bbox = np.array(bboxes[key]) #(num_cam, num_box, 4)

        #new_boxes, new_probs = non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)
        #new_boxes_all_cam, new_probs = bbox, np.array(probs[key])
        new_boxes_all_cam, new_probs = non_max_suppression_fast_multi_cam(bbox, np.array(probs[key]), overlap_thresh=0.5)
        instance_to_color = [np.random.randint(0, 255, 3) for _ in range(len(new_probs))]
        for cam_idx in range(C.num_cam) : 
            img = img_list[cam_idx]
            new_boxes = new_boxes_all_cam[cam_idx] #(num_box, 4)
            for jk in range(new_boxes.shape[0]):
                (x1, y1, x2, y2) = new_boxes[jk,:]
                if(x1 == -C.rpn_stride) :
                    continue 
                # Calculate real coordinates on original image
                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)

                color = (int(instance_to_color[jk][0]), int(instance_to_color[jk][1]), int(instance_to_color[jk][2])) 
                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), color, 4)
                textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))
                all_dets.append((key,100*new_probs[jk]))
                (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)
                textOrg = (real_x1, real_y1-0)
                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)
                cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)
                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)
    print(all_dets)
    conc_img = get_concat_img(img_list)
    save_file_name = os.path.basename(test_img_data[0]['filepath'])
    save_path = os.path.join(result_dir, save_file_name)
    cv2.imwrite(save_path, conc_img) 

def demo_loop(test_path, loop_num, model_rpn, model_view_invariant, model_classifier, bbox_threshold, C, result_dir):
    test_imgs, _, _ = get_data(test_path, C.num_cam)
    random.seed(1)
    random.shuffle(test_imgs)
    imgs_datas = []
    #for i in range(loop_num):
    for i in range(len(test_imgs)):
        test_img_data = test_imgs[i] 
        #st = time.time()
        demo(test_img_data, model_rpn, model_view_invariant, model_classifier, bbox_threshold, C, result_dir)
        #print('Elapsed time = {}'.format(time.time() - st))

def get_map(pred, gt, f):
	T = {}
	P = {}
	fx, fy = f

	for bbox in gt:
		bbox['bbox_matched'] = False

	pred_probs = np.array([s['prob'] for s in pred])
	box_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]

	for box_idx in box_idx_sorted_by_prob:
		pred_box = pred[box_idx]
		pred_class = pred_box['class']
		pred_x1 = pred_box['x1']
		pred_x2 = pred_box['x2']
		pred_y1 = pred_box['y1']
		pred_y2 = pred_box['y2']
		pred_prob = pred_box['prob']
		if pred_class not in P:
			P[pred_class] = []
			T[pred_class] = []
		P[pred_class].append(pred_prob)
		found_match = False

		for gt_box in gt:
			gt_class = gt_box['class']
			gt_x1 = gt_box['x1']/fx
			gt_x2 = gt_box['x2']/fx
			gt_y1 = gt_box['y1']/fy
			gt_y2 = gt_box['y2']/fy
			gt_seen = gt_box['bbox_matched']
			if gt_class != pred_class:
				continue
			if gt_seen:
				continue
			iou_map = iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))
			if iou_map >= 0.5:
				found_match = True
				gt_box['bbox_matched'] = True
				break
			else:
				continue

		T[pred_class].append(int(found_match))

	for gt_box in gt:
		if not gt_box['bbox_matched']:# and not gt_box['difficult']:
			if gt_box['class'] not in P:
				P[gt_box['class']] = []
				T[gt_box['class']] = []

			T[gt_box['class']].append(1)
			P[gt_box['class']].append(0)

	#import pdb
	#pdb.set_trace()
	return T, P

def calc_map_from_model(test_path, model, save_dir = None) : 
    # turn off any data augmentation at test time
    C.use_horizontal_flips = False
    C.use_vertical_flips = False
    C.rot_90 = False
    C.num_features = 512

    record_df = pd.read_csv(C.record_path)

    input_shape_img = (None, None, 3)
    input_shape_features = (None, None, C.num_features)
    num_classes = len(C.class_mapping)
    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
    model_rpn, model_view_invariant, model_classifier = model

    # This might takes a while to parser the data
    test_imgs, _, _ = get_data(test_path, C.num_cam)

    T = {}
    P = {}
    mAPs = []
    #elapsed_time = []
    class_mapping = get_class_mapping(C)
    for idx, test_img_data in enumerate(test_imgs):
        #print('{}/{}'.format(idx,len(test_imgs)))

        #st = time.time()
        img_list = get_img_list(test_img_data, C)
        X_list, ratio, fx, fy = get_X_list(img_list, C)
        bboxes, probs = get_bboxes_probs_from_train_model(X_list, model_rpn, model_view_invariant, model_classifier, 0, class_mapping, C, is_demo=0)

        all_dets = [[] for _ in range(C.num_cam)]
        for key in bboxes:
            bbox = np.array(bboxes[key]) #(num_cam, num_box, 4)
            #new_boxes_all_cam, new_probs = bbox, np.array(probs[key])
            new_boxes_all_cam, new_probs = non_max_suppression_fast_multi_cam(bbox, np.array(probs[key]), overlap_thresh=0.5)
            for cam_idx in range(C.num_cam) : 
                img = img_list[cam_idx]
                new_boxes = new_boxes_all_cam[cam_idx] #(num_box, 4)
                for jk in range(new_boxes.shape[0]):
                    (x1, y1, x2, y2) = new_boxes[jk,:]
                    if(x1 == -C.rpn_stride) :
                        continue 
                    det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}
                    all_dets[cam_idx].append(det)

        #elapsed_time.append(time.time() - st)
        #print('Elapsed time = {}'.format(time.time() - st))

        for cam_idx in range(C.num_cam) : 
            dets = all_dets[cam_idx]
            gt = test_img_data[cam_idx]['bboxes']
            t, p = get_map(dets, gt, (fx, fy))
            for key in t.keys():
                if key not in T:
                    T[key] = []
                    P[key] = []
                T[key].extend(t[key])
                P[key].extend(p[key])
            if(save_dir):
                os.makedirs(save_dir, exist_ok=True) 
                save_name = os.path.basename(test_img_data[cam_idx]['filepath'])
                save_path = os.path.join(save_dir, save_name)

                draw_gt_pred(img_list[cam_idx], dets, gt, ratio, save_path)

    all_aps = []
    for key in T.keys():
        ap = average_precision_score(T[key], P[key])
        #print('{} AP: {}'.format(key, ap))
        ap = 0 if ap==1.0 else ap
        all_aps.append(ap)
    mAP = np.mean(np.array(all_aps))
    #print('mAP = {}'.format(mAP))
    #print('elapsed_time per one scene = {}'.format(np.mean(np.array(elapsed_time))))
    return mAP



def calc_map(test_path, save_dir = None, model = None, model_path = None) : 
    # turn off any data augmentation at test time
    C.use_horizontal_flips = False
    C.use_vertical_flips = False
    C.rot_90 = False
    C.num_features = 512

    record_df = pd.read_csv(C.record_path)

    input_shape_img = (None, None, 3)
    input_shape_features = (None, None, C.num_features)
    num_classes = len(C.class_mapping)
    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
    if model :
        model_rpn, model_view_invariant, model_classifier = model
    elif model_path :
        model_rpn, model_view_invariant, model_classifier = get_test_model(input_shape_img, input_shape_features, C.num_cam, C.num_rois, num_anchors, num_classes, model_path)
    else : 
        model_rpn, model_view_invariant, model_classifier = get_test_model(input_shape_img, input_shape_features, C.num_cam, C.num_rois, num_anchors, num_classes, C.best_model_path)

    # This might takes a while to parser the data
    test_imgs, _, _ = get_data(test_path, C.num_cam)

    T = {}
    P = {}
    mAPs = []
    elapsed_time = []
    class_mapping = get_class_mapping(C)
    print('strat calc map')
    for idx, test_img_data in enumerate(test_imgs):
        #print('{}/{}'.format(idx,len(test_imgs)))

        st = time.time()
        img_list = get_img_list(test_img_data, C)
        X_list, ratio, fx, fy = get_X_list(img_list, C)
        bboxes, probs = get_bboxes_probs(X_list, model_rpn, model_view_invariant, model_classifier, 0, class_mapping, C, is_demo=0)

        all_dets = [[] for _ in range(C.num_cam)]
        for key in bboxes:
            bbox = np.array(bboxes[key]) #(num_cam, num_box, 4)
            #new_boxes_all_cam, new_probs = bbox, np.array(probs[key])
            new_boxes_all_cam, new_probs = non_max_suppression_fast_multi_cam(bbox, np.array(probs[key]), overlap_thresh=0.5)
            for cam_idx in range(C.num_cam) : 
                img = img_list[cam_idx]
                new_boxes = new_boxes_all_cam[cam_idx] #(num_box, 4)
                for jk in range(new_boxes.shape[0]):
                    (x1, y1, x2, y2) = new_boxes[jk,:]
                    if(x1 == -C.rpn_stride) :
                        continue 
                    det = {'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class': key, 'prob': new_probs[jk]}
                    all_dets[cam_idx].append(det)

        elapsed_time.append(time.time() - st)
        #print('Elapsed time = {}'.format(time.time() - st))

        print(all_dets)
        for cam_idx in range(C.num_cam) : 
            dets = all_dets[cam_idx]
            gt = test_img_data[cam_idx]['bboxes']
            t, p = get_map(dets, gt, (fx, fy))
            for key in t.keys():
                if key not in T:
                    T[key] = []
                    P[key] = []
                T[key].extend(t[key])
                P[key].extend(p[key])
            if(save_dir):
                os.makedirs(save_dir, exist_ok=True) 
                save_name = os.path.basename(test_img_data[cam_idx]['filepath'])
                save_path = os.path.join(save_dir, save_name)

                draw_gt_pred(img_list[cam_idx], dets, gt, ratio, save_path)

    all_aps = []
    for key in T.keys():
        ap = average_precision_score(T[key], P[key])
        ap = 0 if ap==1.0 else ap
        print('{} AP: {}'.format(key, ap))
        all_aps.append(ap)
    mAP = np.mean(np.array(all_aps))
    print('mAP = {}'.format(mAP))
    print('elapsed_time per one scene = {}'.format(np.mean(np.array(elapsed_time))))
    return mAP


'''
def show_demos(test_path, bbox_threshold, num_demo):
    # turn off any data augmentation at test time
    C.use_horizontal_flips = False
    C.use_vertical_flips = False
    C.rot_90 = False
    C.num_features = 512

    input_shape_img = (None, None, 3)
    input_shape_features = (None, None, C.num_features)
    num_classes = len(C.class_mapping)
    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
    model_rpn, model_classifier = get_test_model(input_shape_img, input_shape_features, C.num_cam, C.num_rois, num_anchors, num_classes, C.model_path)

    demo_loop(test_path, num_demo, model_rpn, model_classifier, bbox_threshold, C)
'''

def show_demos(test_path, bbox_threshold, num_demo, result_img_path):
    test_result_dir = os.path.join(result_img_path, 'testset')
    os.makedirs(test_result_dir, exist_ok = True)
    # turn off any data augmentation at test time
    C.use_horizontal_flips = False
    C.use_vertical_flips = False
    C.rot_90 = False
    C.num_features = 512

    input_shape_img = (None, None, 3)
    input_shape_features = (None, None, C.num_features)
    num_classes = len(C.class_mapping)
    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
    model_rpn, model_view_invariant, model_classifier = get_test_model(input_shape_img, input_shape_features, C.num_cam, C.num_rois, num_anchors, num_classes, C.best_model_path)

    demo_loop(test_path, num_demo, model_rpn, model_view_invariant, model_classifier, bbox_threshold, C, test_result_dir)

def get_data(input_path, num_cam):
    """Parse the data from annotation file
    
    Args:
        input_path: annotation file path
        C : config

    Returns:
        all_data: list of list(filepath, width, height, list(bboxes))
        classes_count: dict{key:class_name, value:count_num} 
            e.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}
        class_mapping: dict{key:class_name, value: idx}
            e.g. {'Car': 0, 'Mobile phone': 1, 'Person': 2}
    """
    found_bg = False
    all_imgs = {}

    classes_count = {}

    class_mapping = {}

    visualise = True

    i = 1
    
    with open(input_path,'r') as f:

        print('Parsing annotation files')

        for line in f:
            # Print process
            sys.stdout.write('\r'+'idx=' + str(i))
            i += 1
            
            line_split = line.strip().split(',')
            labels_in_cam = []
            ref_name = line_split[0]
            if ref_name not in all_imgs:
                all_imgs[ref_name] = {}

            for i in range(num_cam):
                # Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)
                # Note:
                #   One path_filename might has several classes (class_name)
                #   x1, y1, x2, y2 are the pixel value of the origial image, not the ratio value
                #   (x1, y1) top left coordinates; (x2, y2) bottom right coordinates
                #   x1,y1-------------------
                #   |                       |
                #   |                       |
                #   |                       |
                #   |                       |
                #   ---------------------x2,y2
                (filename,x1,y1,x2,y2,class_name) = line_split[i*6:(i+1)*6]

                if class_name not in classes_count:
                    classes_count[class_name] = 1
                else:
                    classes_count[class_name] += 1

                if class_name not in class_mapping:
                    if class_name == 'bg' and found_bg == False:
                        print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')
                        found_bg = True
                    class_mapping[class_name] = len(class_mapping)

                if filename not in all_imgs[ref_name]:
                    all_imgs[ref_name][filename] = {}
                    
                    img = cv2.imread(filename)
                    (rows,cols) = img.shape[:2]
                    all_imgs[ref_name][filename]['filepath'] = filename
                    all_imgs[ref_name][filename]['width'] = cols
                    all_imgs[ref_name][filename]['height'] = rows
                    all_imgs[ref_name][filename]['bboxes'] = []

                all_imgs[ref_name][filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})


        all_data = []
        for key in all_imgs:
            scene = []
            for key2 in all_imgs[key] :
                scene.append(all_imgs[key][key2])
            all_data.append(scene)
        
        # make sure the bg class is last in the list
        if found_bg:
            if class_mapping['bg'] != len(class_mapping) - 1:
                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]
                val_to_switch = class_mapping['bg']
                class_mapping['bg'] = len(class_mapping) - 1
                class_mapping[key_to_switch] = val_to_switch
        
        return all_data, classes_count, class_mapping

class RoiPoolingConv(Layer):
    '''ROI pooling layer for 2D inputs.
    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,
    K. He, X. Zhang, S. Ren, J. Sun
    # Arguments
        pool_size: int
            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.
        num_rois: number of regions of interest to be used
    # Input shape
        list of two 4D tensors [X_img,X_roi] with shape:
        X_img:
        `(1, rows, cols, channels)`
        X_roi:
        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)
    # Output shape
        3D tensor with shape:
        `(1, num_rois, channels, pool_size, pool_size)`
    '''
    def __init__(self, pool_size, num_rois, **kwargs):

        #self.dim_ordering = K.image_dim_ordering()
        #self.dim_ordering = K.common.image_dim_ordering()
        self.dim_ordering = K.image_data_format()
        self.pool_size = pool_size
        self.num_rois = num_rois

        super(RoiPoolingConv, self).__init__(**kwargs)

    def build(self, input_shape):
        self.nb_channels = input_shape[0][3]   

    def compute_output_shape(self, input_shape):
        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels

    def call(self, x, mask=None):

        assert(len(x) == 2)

        # x[0] is image with shape (rows, cols, channels)
        img = x[0]

        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)
        rois = x[1]

        input_shape = K.shape(img)

        outputs = []

        for roi_idx in range(self.num_rois):

            x = rois[0, roi_idx, 0]
            y = rois[0, roi_idx, 1]
            w = rois[0, roi_idx, 2]
            h = rois[0, roi_idx, 3]

            x = K.cast(x, 'int32')
            y = K.cast(y, 'int32')
            w = K.cast(w, 'int32')
            h = K.cast(h, 'int32')
            
            # Resized roi of the image to pooling size (7x7)
            roi = K.switch(K.equal(x, -1), tf.zeros([self.pool_size, self.pool_size, self.nb_channels], tf.float32), img[0, y:y+h, x:x+w, :])
            rs = tf.image.resize_images(roi, (self.pool_size, self.pool_size))
            rs = K.reshape(rs, (1, self.pool_size, self.pool_size, self.nb_channels))
            #rs = K.reshape(rs, (1, 1, -1))
            outputs.append(rs)

        final_output = K.concatenate(outputs, axis=0)
        #final_output = tf.keras.layers.Concatenate(axis=0)(outputs) 

        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)
        # Might be (1, 4, 7, 7, 3)
        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))

        # permute_dimensions is similar to transpose
        #final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))

        return final_output
    
    
    def get_config(self):
        config = {'pool_size': self.pool_size,
                  'num_rois': self.num_rois}
        base_config = super(RoiPoolingConv, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

def get_img_output_length(width, height):
    def get_output_length(input_length):
        return input_length//16

    return get_output_length(width), get_output_length(height)    

def nn_base_model():
    model = Sequential(name='nn_base')

    # Block 1
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1'))
    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))

    # Block 2
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1'))
    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))

    # Block 3
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1'))
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2'))
    model.add(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))

    # Block 4
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1'))
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2'))
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3'))
    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))

    # Block 5
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1'))
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'))
    model.add(Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'))
    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

    return model

def view_invariant_layer_model(H, W, num_anchors, view_invar_feature_size):
    """Create a view invariant layer
        Step1: Pass through the feature map from rpn body layer to convolutional layers
                Keep the padding 'same' to preserve the feature map's size
        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer. num_anchors*view_invar_feature_size (9*128 in here) channels for 0~1 sigmoid view invariant feature
    Args:
        num_anchors: 9 in here
        view_invar_feature_size

    Returns:
        view invariant feature 
    """
    model = Sequential(name='vi_layer')
    model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='vi_conv1'))
    model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='vi_conv2'))
    model.add(Conv2D(num_anchors * view_invar_feature_size, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='vi_out'))
    model.add(Reshape((H, W, num_anchors, view_invar_feature_size), name='vi_reshape'))
    model.add(Lambda(lambda x: K.l2_normalize(x, -1), name='vi_l2_norm'))
    model.add(Lambda(lambda x: K.expand_dims(x, 1), name='vi_expand_dim'))
    return model

def view_invariant_conc_layer(view_invariants):
    #view_invariants x : list of (None, 1, H, W, C)
    return Concatenate(axis=1, name='vi_pooling')(view_invariants)
    
def rpn_layer_model(num_anchors):
    """Create a rpn layer
        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer
                Keep the padding 'same' to preserve the feature map's size
        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer
                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output
                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation
    Args:
        base_layers: vgg in here
        num_anchors: 9 in here

    Returns:
        [x_class, x_regr, base_layers]
        x_class: classification for whether it's an object
        x_regr: bboxes regression
        base_layers: vgg in here
    """
    rpn_share = Sequential(name='rpn_body')
    rpn_share.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1'))
    model_class = Sequential(name='rpn_cls')
    model_regr = Sequential(name='rpn_regr')

    model_class.add(Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class'))
    model_regr.add(Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress'))

    return rpn_share, model_class, model_regr

def rpn_layer(base_layers, num_anchors):
    """Create a rpn layer
        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer
                Keep the padding 'same' to preserve the feature map's size
        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer
                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output
                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation
    Args:
        base_layers: vgg in here
        num_anchors: 9 in here

    Returns:
        [x_class, x_regr, base_layers]
        x_class: classification for whether it's an object
        x_regr: bboxes regression
        base_layers: vgg in here
    """
    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)

    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)
    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)

    return [x_class, x_regr, base_layers]

#def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):
def classifier_layer(base_layers, input_rois, num_rois, num_feat, num_cam, nb_classes = 4):
    """Create a classifier layer
    
    Args:
        base_layers: list(vgg)
        input_rois: list(`(1,num_rois,4)` list of rois, with ordering (x,y,w,h))
        num_rois: number of rois to be processed in one time (4 in here)

    Returns:
        list(out_class, out_regr)
        out_class: classifier layer output
        out_regr: regression layer output
    """

    input_shape = (num_rois,7,7,512)
    pooling_regions = 7

    # num_rois (4) 7x7 roi pooling
    reduce_channel = Conv2D(num_feat//num_cam, (3, 3), activation='relu', padding='same', name='reduce_channel')
    out_roi_pools = []
    for i in range(num_cam):
        reduced_base_layer = reduce_channel(base_layers[i])
        out_roi_pools.append(RoiPoolingConv(pooling_regions, num_rois)([reduced_base_layer, input_rois[i]])) #(1, 4, 7, 7, 512)
        #out_roi_pools.append(RoiPoolingConv(pooling_regions, num_rois)([base_layers[i], input_rois[i]])) #(1, 4, 7, 7, 512)
    out_roi_pool = Concatenate(axis=-1, name='ViewPooling')(out_roi_pools)
    out_roi_pool_flat = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)
    # Flatten the convlutional layer and connected to 2 FC and 2 dropout
    #out = TimeDistributed(Conv2D(512, (3, 3), activation='relu', padding='same', name='classifier_conv1'))(out_roi_pool)
    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out_roi_pool_flat)
    out = TimeDistributed(Dropout(0.5))(out)
    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)
    out = TimeDistributed(Dropout(0.5))(out)

    # There are two output layer
    # out_class: softmax acivation function for classify the class name of the object
    # out_regr: linear activation function for bboxes coordinates regression
    # note: no regression target for bg class
    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)
    out_regr = TimeDistributed(Dense(num_cam* 4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)

    return [out_class, out_regr]

def union(au, bu, area_intersection):
    area_a = (au[2] - au[0]) * (au[3] - au[1])
    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])
    area_union = area_a + area_b - area_intersection
    return area_union

def intersection(ai, bi):
    x = max(ai[0], bi[0])
    y = max(ai[1], bi[1])
    w = min(ai[2], bi[2]) - x
    h = min(ai[3], bi[3]) - y
    if w < 0 or h < 0:
        return 0
    return w*h

def iou(a, b):
    # a and b should be (x1,y1,x2,y2)

    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:
        return 0.0

    area_i = intersection(a, b)
    area_u = union(a, b, area_i)

    return float(area_i) / float(area_u + 1e-6)

def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):
    """(Important part!) Calculate the rpn for all anchors 
        If feature map has shape 38x50=1900, there are 1900x9=17100 potential anchors
    
    Args:
        C: config
        img_data: augmented image data
        width: original image width (e.g. 600)
        height: original image height (e.g. 800)
        resized_width: resized image width according to C.im_size (e.g. 300)
        resized_height: resized image height according to C.im_size (e.g. 400)
        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size

    Returns:
        y_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)
            y_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)
            y_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)
        y_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)
            y_rpn_regr: x1,y1,x2,y2 bunding boxes coordinates
    """
    downscale = float(C.rpn_stride) 
    anchor_sizes = C.anchor_box_scales   # 128, 256, 512
    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1
    num_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9

    # calculate the output map size based on the network architecture
    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)

    n_anchratios = len(anchor_ratios)    # 3
    
    # initialise empty output objectives
    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))
    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))
    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))

    num_bboxes = len(img_data['bboxes'])

    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)
    best_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)
    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)
    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)
    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)

    # get the GT box coordinates, and resize to account for image resizing
    gta = np.zeros((num_bboxes, 4))
    for bbox_num, bbox in enumerate(img_data['bboxes']):
        # get the GT box coordinates, and resize to account for image resizing
        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))
        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))
        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))
        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))
    
    # rpn ground truth

    for anchor_size_idx in range(len(anchor_sizes)):
        for anchor_ratio_idx in range(n_anchratios):
            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]
            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]   
            
            for ix in range(output_width):                  
                # x-coordinates of the current anchor box   
                x1_anc = downscale * (ix + 0.5) - anchor_x / 2
                x2_anc = downscale * (ix + 0.5) + anchor_x / 2  
                
                # ignore boxes that go across image boundaries                  
                if x1_anc < 0 or x2_anc > resized_width:
                    continue
                    
                for jy in range(output_height):

                    # y-coordinates of the current anchor box
                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2
                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2

                    # ignore boxes that go across image boundaries
                    if y1_anc < 0 or y2_anc > resized_height:
                        continue

                    # bbox_type indicates whether an anchor should be a target
                    # Initialize with 'negative'
                    bbox_type = 'neg'

                    # this is the best IOU for the (x,y) coord and the current anchor
                    # note that this is different from the best IOU for a GT bbox
                    best_iou_for_loc = 0.0

                    for bbox_num in range(num_bboxes):
                        
                        # get IOU of the current GT box and the current anchor box
                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])
                        # calculate the regression targets if they will be needed
                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:
                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0
                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0
                            cxa = (x1_anc + x2_anc)/2.0
                            cya = (y1_anc + y2_anc)/2.0

                            # x,y are the center point of ground-truth bbox
                            # xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))
                            # w,h are the width and height of ground-truth bbox
                            # wa,ha are the width and height of anchor bboxe
                            # tx = (x - xa) / wa
                            # ty = (y - ya) / ha
                            # tw = log(w / wa)
                            # th = log(h / ha)
                            tx = (cx - cxa) / (x2_anc - x1_anc)
                            ty = (cy - cya) / (y2_anc - y1_anc)
                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))
                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))
                        
                        if img_data['bboxes'][bbox_num]['class'] != 'bg':

                            # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best
                            if curr_iou > best_iou_for_bbox[bbox_num]:
                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]
                                best_iou_for_bbox[bbox_num] = curr_iou
                                best_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]
                                best_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]

                            # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)
                            if curr_iou > C.rpn_max_overlap:
                                bbox_type = 'pos'
                                num_anchors_for_bbox[bbox_num] += 1
                                # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position
                                if curr_iou > best_iou_for_loc:
                                    best_iou_for_loc = curr_iou
                                    best_regr = (tx, ty, tw, th)

                            # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective
                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:
                                # gray zone between neg and pos
                                if bbox_type != 'pos':
                                    bbox_type = 'neutral'

                    # turn on or off outputs depending on IOUs
                    if bbox_type == 'neg':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                    elif bbox_type == 'neutral':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                    elif bbox_type == 'pos':
                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)
                        y_rpn_regr[jy, ix, start:start+4] = best_regr

    # we ensure that every bbox has at least one positive RPN region

    for idx in range(num_anchors_for_bbox.shape[0]):
        if num_anchors_for_bbox[idx] == 0:
            # no box with an IOU greater than zero ...
            if best_anchor_for_bbox[idx, 0] == -1:
                continue
            y_is_box_valid[
                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *
                best_anchor_for_bbox[idx,3]] = 1
            y_rpn_overlap[
                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *
                best_anchor_for_bbox[idx,3]] = 1
            start = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])
            y_rpn_regr[
                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]

    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))
    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)

    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))
    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)

    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))
    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)

    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))
    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))

    num_pos = len(pos_locs[0])

    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative
    # regions. We also limit it to 256 regions.
    num_regions = 256

    if len(pos_locs[0]) > num_regions/2:
        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)
        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0
        num_pos = num_regions/2

    if len(neg_locs[0]) + num_pos > num_regions:
        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)
        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0

    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)
    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)

    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos

def get_new_img_size(width, height, img_min_side=300):
    if width <= height:
        f = float(img_min_side) / width
        resized_height = int(f * height)
        resized_width = img_min_side
    else:
        f = float(img_min_side) / height
        resized_width = int(f * width)
        resized_height = img_min_side

    return resized_width, resized_height

def augment(img_data, config, augment=True):
    assert 'filepath' in img_data
    assert 'bboxes' in img_data
    assert 'width' in img_data
    assert 'height' in img_data

    img_data_aug = copy.deepcopy(img_data)

    img = cv2.imread(img_data_aug['filepath'])

    if augment:
        rows, cols = img.shape[:2]

        if config.use_horizontal_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 1)
            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                bbox['x2'] = cols - x1
                bbox['x1'] = cols - x2

        if config.use_vertical_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 0)
            for bbox in img_data_aug['bboxes']:
                y1 = bbox['y1']
                y2 = bbox['y2']
                bbox['y2'] = rows - y1
                bbox['y1'] = rows - y2

        if config.rot_90:
            angle = np.random.choice([0,90,180,270],1)[0]
            if angle == 270:
                img = np.transpose(img, (1,0,2))
                img = cv2.flip(img, 0)
            elif angle == 180:
                img = cv2.flip(img, -1)
            elif angle == 90:
                img = np.transpose(img, (1,0,2))
                img = cv2.flip(img, 1)
            elif angle == 0:
                pass

            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                y1 = bbox['y1']
                y2 = bbox['y2']
                if angle == 270:
                    bbox['x1'] = y1
                    bbox['x2'] = y2
                    bbox['y1'] = cols - x2
                    bbox['y2'] = cols - x1
                elif angle == 180:
                    bbox['x2'] = cols - x1
                    bbox['x1'] = cols - x2
                    bbox['y2'] = rows - y1
                    bbox['y1'] = rows - y2
                elif angle == 90:
                    bbox['x1'] = rows - y2
                    bbox['x2'] = rows - y1
                    bbox['y1'] = x1
                    bbox['y2'] = x2        
                elif angle == 0:
                    pass

    img_data_aug['width'] = img.shape[1]
    img_data_aug['height'] = img.shape[0]
    return img_data_aug, img

def get_anchor_gt_thread(all_img_data, C, img_length_calc_function, mode='train'):
    """ Yield the ground-truth anchors as Y (labels)
        
    Args:
        all_img_data: list(filepath, width, height, list(bboxes))
        C: config
        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size
        mode: 'train' or 'test'; 'train' mode need augmentation

    Returns:
        x_img: image data after resized and scaling (smallest size = 300px)
        Y: [y_rpn_cls, y_rpn_regr]
        img_data_aug: augmented image data (original image with augmentation)
        debug_img: show image for debug
        num_pos: show number of positive anchors for debug
    """
    def img_processing(img_data, C, img_length_calc_function, mode, x_img_list, y_rpn_list, img_data_aug_list, debug_img_list, num_pos_list) : 
        try:
            # read in image, and optionally add augmentation
            if mode == 'train':
                img_data_aug, x_img = augment(img_data, C, augment=True)
            else:
                img_data_aug, x_img = augment(img_data, C, augment=False)
            (width, height) = (img_data_aug['width'], img_data_aug['height'])
            (rows, cols, _) = x_img.shape

            assert cols == width
            assert rows == height

            # get image dimensions for resizing
            (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)

            # resize the image so that smalles side is length = 300px
            x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)
            debug_img = x_img.copy()

            y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)

            # Zero-center by mean pixel, and preprocess image
            x_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB
            x_img = x_img.astype(np.float32)
            x_img[:, :, 0] -= C.img_channel_mean[0]
            x_img[:, :, 1] -= C.img_channel_mean[1]
            x_img[:, :, 2] -= C.img_channel_mean[2]
            x_img /= C.img_scaling_factor

            x_img = np.transpose(x_img, (2, 0, 1))
            x_img = np.expand_dims(x_img, axis=0)

            y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling

            x_img = np.transpose(x_img, (0, 2, 3, 1))
            y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))
            y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))
            
            x_img_list.append(np.copy(x_img))
            y_rpn_list.extend([np.copy(y_rpn_cls), np.copy(y_rpn_regr)])
            img_data_aug_list.append(img_data_aug)
            debug_img_list.append(debug_img)
            num_pos_list.append(num_pos)

        except Exception as e:
            print(e)

    num_cam = C.num_cam
    while True:
        for mv_img_data in all_img_data:
            x_img_list, y_rpn_list, img_data_aug_list, debug_img_list, num_pos_list = [[], [], []], [[], [], []], [[], [], []], [[], [], []], [[], [], []]
            threads = []
            for i, img_data in enumerate(mv_img_data) : 
                t = threading.Thread(target=img_processing, args=(img_data, C, img_length_calc_function, mode, x_img_list[i], y_rpn_list[i], img_data_aug_list[i], debug_img_list[i], num_pos_list[i]))
                threads.append(t)
                t.start()
            for t in threads :
                t.join()
            
            x_img_list = [i[0] for i in x_img_list]
            y_rpn_list_1d = []
            for y_rpn in y_rpn_list : y_rpn_list_1d.extend(y_rpn)
            img_data_aug_list = [i[0] for i in img_data_aug_list]
            debug_img_list = [i[0] for i in debug_img_list]
            num_pos_list = [i[0] for i in num_pos_list]

            if len(x_img_list) != num_cam : continue
            yield x_img_list, y_rpn_list_1d, img_data_aug_list, debug_img_list, num_pos_list

def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train'):
    """ Yield the ground-truth anchors as Y (labels)
        
    Args:
        all_img_data: list(filepath, width, height, list(bboxes))
        C: config
        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size
        mode: 'train' or 'test'; 'train' mode need augmentation

    Returns:
        x_img: image data after resized and scaling (smallest size = 300px)
        Y: [y_rpn_cls, y_rpn_regr]
        img_data_aug: augmented image data (original image with augmentation)
        debug_img: show image for debug
        num_pos: show number of positive anchors for debug
    """
    num_cam = C.num_cam
    while True:
        for mv_img_data in all_img_data:
            x_img_list, y_rpn_list, img_data_aug_list, debug_img_list, num_pos_list = [], [], [], [], []
            for img_data in mv_img_data: 
                try:
                    # read in image, and optionally add augmentation
                    if mode == 'train':
                        img_data_aug, x_img = augment(img_data, C, augment=True)
                    else:
                        img_data_aug, x_img = augment(img_data, C, augment=False)
                    (width, height) = (img_data_aug['width'], img_data_aug['height'])
                    (rows, cols, _) = x_img.shape

                    assert cols == width
                    assert rows == height

                    # get image dimensions for resizing
                    (resized_width, resized_height) = get_new_img_size(width, height, C.im_size)

                    # resize the image so that smalles side is length = 300px
                    x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)
                    debug_img = x_img.copy()

                    try:
                        y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(C, img_data_aug, width, height, resized_width, resized_height, img_length_calc_function)
                    except:
                        break 

                    # Zero-center by mean pixel, and preprocess image
                    x_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB
                    x_img = x_img.astype(np.float32)
                    x_img[:, :, 0] -= C.img_channel_mean[0]
                    x_img[:, :, 1] -= C.img_channel_mean[1]
                    x_img[:, :, 2] -= C.img_channel_mean[2]
                    x_img /= C.img_scaling_factor

                    x_img = np.transpose(x_img, (2, 0, 1))
                    x_img = np.expand_dims(x_img, axis=0)

                    y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling

                    x_img = np.transpose(x_img, (0, 2, 3, 1))
                    y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))
                    y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))
                    
                    x_img_list.append(np.copy(x_img))
                    y_rpn_list.extend([np.copy(y_rpn_cls), np.copy(y_rpn_regr)])
                    img_data_aug_list.append(img_data_aug)
                    debug_img_list.append(debug_img)
                    num_pos_list.append(num_pos)

                except Exception as e:
                    print(e)
                    break

            if len(x_img_list) != num_cam : continue
            yield x_img_list, y_rpn_list, img_data_aug_list, debug_img_list, num_pos_list


lambda_rpn_regr = 1.0
lambda_rpn_class = 1.0

lambda_cls_regr = 1.0
lambda_cls_class = 1.0

epsilon = 1e-4

def rpn_loss_regr(num_anchors):
    """Loss function for rpn regression
    Args:
        num_anchors: number of anchors (9 in here)
    Returns:
        Smooth L1 loss function 
                           0.5*x*x (if x_abs < 1)
                           x_abx - 0.5 (otherwise)
    """
    def rpn_loss_regr_fixed_num(y_true, y_pred):

        # x is the difference between true value and predicted vaue
        x = y_true[:, :, :, 4 * num_anchors:] - y_pred

        # absolute value of x
        x_abs = K.abs(x)

        # If x_abs <= 1.0, x_bool = 1
        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)

        return lambda_rpn_regr * K.sum(
            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])

    return rpn_loss_regr_fixed_num


def rpn_loss_cls(num_anchors):
    """Loss function for rpn classification
    Args:
        num_anchors: number of anchors (9 in here)
        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid
        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative
    Returns:
        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N
    """
    def rpn_loss_cls_fixed_num(y_true, y_pred):

            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])

    return rpn_loss_cls_fixed_num

def view_invariant_loss(alpha=.3):
    """Loss function for rpn classification
    Args:
        y_pred: view invariant features, shape == (1, num_cam, H, W, A, vi_featue_size)
        y_true: anchor_target idx, shape == (1, 2, num_GT, 4)
            first : batch_size(dummy)
            second : anchor or target ?
            third : GT idx
            fourth : cam, H, W, A
    Returns:
    """
    def triplet_loss_func(y_true, y_pred):
        y_true = y_true[0, :, :, :, 0, 0]
        y_true = tf.cast(y_true, 'int32') #(2, numSample, 4)
        anchor_idx = y_true[0] #(numSample, 4)
        pos_idx = y_true[1] #(numSample, 4)
        neg_idx = y_true[2] #(numSample, 4)

        anchor = tf.gather_nd(y_pred[0], anchor_idx) #(numSample, vi_feature_size)
        positive = tf.gather_nd(y_pred[0], pos_idx) #(numSample, vi_feature_size)
        negative = tf.gather_nd(y_pred[0], neg_idx) #(numSample, vi_feature_size)

        positive_dist = tf.sqrt(tf.reduce_sum(tf.square(anchor - positive), -1)) #(numSample, )
        negative_dist = tf.sqrt(tf.reduce_sum(tf.square(anchor - negative), -1)) #(numSample, )

        loss_1 = positive_dist - negative_dist + alpha
        loss = tf.reduce_sum(tf.maximum(loss_1, 0.0)) #()

        return loss
    return triplet_loss_func

#def class_loss_regr(num_classes):
def class_loss_regr(num_classes, num_cam):
    """Loss function for rpn regression
    Args:
        num_anchors: number of anchors (9 in here)
        num_cam : number of cam (3 in here)
    Returns:
        Smooth L1 loss function 
                           0.5*x*x (if x_abs < 1)
                           x_abx - 0.5 (otherwise)
    """
    def class_loss_regr_fixed_num(y_true, y_pred):
        #x = y_true[:, :, 4*num_classes:] - y_pred
        x = y_true[:, :, num_cam*4*num_classes:] - y_pred
        x_abs = K.abs(x)
        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')
        #return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])
        return lambda_cls_regr * K.sum(y_true[:, :, :num_cam*4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :num_cam*4*num_classes])
    return class_loss_regr_fixed_num


def class_loss_cls(y_true, y_pred):
    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))

def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):
    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/
    # if there are no boxes, return an empty list

    # Process explanation:
    #   Step 1: Sort the probs list
    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list
    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list
    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list 
    if len(boxes) == 0:
        return []

    # grab the coordinates of the bounding boxes
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]

#    np.testing.assert_array_less(x1, x2)
#    np.testing.assert_array_less(y1, y2)

    # if the bounding boxes integers, convert them to floats --
    # this is important since we'll be doing a bunch of divisions
    if boxes.dtype.kind == "i":
        boxes = boxes.astype("float")

    # initialize the list of picked indexes 
    pick = []

    # calculate the areas
    area = (x2 - x1) * (y2 - y1)

    # sort the bounding boxes 
    idxs = np.argsort(probs)

    # sort the bounding boxes 
    invalid_idxs = np.where((x1[idxs] - x2[idxs] >= 0) | (y1[idxs] - y2[idxs] >= 0))
    idxs = np.delete(idxs, invalid_idxs, 0)
    np.testing.assert_array_less(x1[idxs], x2[idxs])
    np.testing.assert_array_less(y1[idxs], y2[idxs])

    # keep looping while some indexes still remain in the indexes
    # list
    while len(idxs) > 0:
        # grab the last index in the indexes list and add the
        # index value to the list of picked indexes
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        # find the intersection

        xx1_int = np.maximum(x1[i], x1[idxs[:last]])
        yy1_int = np.maximum(y1[i], y1[idxs[:last]])
        xx2_int = np.minimum(x2[i], x2[idxs[:last]])
        yy2_int = np.minimum(y2[i], y2[idxs[:last]])

        ww_int = np.maximum(0, xx2_int - xx1_int)
        hh_int = np.maximum(0, yy2_int - yy1_int)

        area_int = ww_int * hh_int

        # find the union
        area_union = area[i] + area[idxs[:last]] - area_int

        # compute the ratio of overlap
        overlap = area_int/(area_union + 1e-6)

        # delete all indexes from the index list that have
        idxs = np.delete(idxs, np.concatenate(([last],
            np.where(overlap > overlap_thresh)[0])))

        if len(pick) >= max_boxes:
            break

    # return only the bounding boxes that were picked using the integer data type
    boxes_idx = pick
    boxes = boxes[pick].astype("int")
    probs = probs[pick]
    return boxes_idx, boxes, probs

def non_max_suppression_fast_multi_cam(boxes, probs, overlap_thresh=0.9, max_boxes=300):
    # boxes : (num_cam, num_box, 4)
    # probs : (num_box, )
    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/
    # if there are no boxes, return an empty list

    # Process explanation:
    #   Step 1: Sort the probs list
    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list
    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list
    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list 
    if len(boxes) == 0:
        return []

    boxes = boxes.transpose(1, 0, 2) #(num_box, num_cam, 4)
    # grab the coordinates of the bounding boxes
    x1 = boxes[:, :, 0] #(num_box, num_cam)
    y1 = boxes[:, :, 1]
    x2 = boxes[:, :, 2]
    y2 = boxes[:, :, 3]

    np.testing.assert_array_less(x1, x2)
    np.testing.assert_array_less(y1, y2)

    # if the bounding boxes integers, convert them to floats --
    # this is important since we'll be doing a bunch of divisions
    if boxes.dtype.kind == "i":
        boxes = boxes.astype("float")

    # initialize the list of picked indexes 
    pick = []

    # calculate the areas 
    area = (x2 - x1) * (y2 - y1) #(num_box, num_cam)

    # sort the bounding boxes 
    idxs = np.argsort(probs) #(num_box,)

    # keep looping while some indexes still remain in the indexes
    # list
    while len(idxs) > 0:
        # grab the last index in the indexes list and add the
        # index value to the list of picked indexes
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)
        # find the intersection
        xx1_int = np.maximum(x1[i], x1[idxs[:last]]) #x1[i]: (num_cam, ), x1[idxs[:last]]: (num_box, num_cam) #out: (num_box, num_cam)
        yy1_int = np.maximum(y1[i], y1[idxs[:last]])
        xx2_int = np.minimum(x2[i], x2[idxs[:last]])
        yy2_int = np.minimum(y2[i], y2[idxs[:last]])

        ww_int = np.maximum(0, xx2_int - xx1_int) #(num_box, num_cam)
        hh_int = np.maximum(0, yy2_int - yy1_int)

        area_int = ww_int * hh_int #(num_box, num_cam)

        # find the union
        area_union = area[i] + area[idxs[:last]] - area_int

        # compute the ratio of overlap
        overlap = area_int/(area_union + 1e-6) #(num_box, num_cam)

        # delete all indexes from the index list that have
        idxs = np.delete(idxs, np.concatenate(([last],
            np.where(np.all(overlap > overlap_thresh, 1))[0])))
            #np.where(np.any(overlap > overlap_thresh, 1))[0])))

        if len(pick) >= max_boxes:
            break

    # return only the bounding boxes that were picked using the integer data type
    boxes = boxes[pick].astype("int").transpose(1, 0, 2) #(num_cam, num_box, 4)
    probs = probs[pick]
    return boxes, probs

def apply_regr_np(X, T):
    """Apply regression layer to all anchors in one feature map

    Args:
        X: shape=(4, 18, 25) the current anchor type for all points in the feature map
        T: regression layer shape=(4, 18, 25)

    Returns:
        X: regressed position and size for current anchor
    """
    try:
        x = X[0, :, :]
        y = X[1, :, :]
        w = X[2, :, :]
        h = X[3, :, :]

        tx = T[0, :, :]
        ty = T[1, :, :]
        tw = T[2, :, :]
        th = T[3, :, :]

        cx = x + w/2.
        cy = y + h/2.
        cx1 = tx * w + cx
        cy1 = ty * h + cy

        w1 = np.exp(tw.astype(np.float64)) * w
        h1 = np.exp(th.astype(np.float64)) * h
        x1 = cx1 - w1/2.
        y1 = cy1 - h1/2.

        x1 = np.round(x1)
        y1 = np.round(y1)
        w1 = np.round(w1)
        h1 = np.round(h1)
        return np.stack([x1, y1, w1, h1])
    except Exception as e:
        print(e)
        return X
    
def apply_regr(x, y, w, h, tx, ty, tw, th):
    # Apply regression to x, y, w and h
    try:
        cx = x + w/2.
        cy = y + h/2.
        cx1 = tx * w + cx
        cy1 = ty * h + cy
        w1 = math.exp(tw) * w
        h1 = math.exp(th) * h
        x1 = cx1 - w1/2.
        y1 = cy1 - h1/2.
        x1 = int(round(x1))
        y1 = int(round(y1))
        w1 = int(round(w1))
        h1 = int(round(h1))

        return x1, y1, w1, h1

    except ValueError:
        return x, y, w, h
    except OverflowError:
        return x, y, w, h
    except Exception as e:
        print(e)
        return x, y, w, h

def calc_iou(grouped_R, img_datas, C, class_mapping):
    """Converts from (x1,y1,x2,y2) to (x,y,w,h) format

    Args:
        Grouped_R: [bboxes, bboxes, ...], len(Grouped_R) = num_cam
            bboxes : #(300, 4)  
    """
    bboxes_list = [img_data['bboxes'] for img_data in img_datas] #[[cam1_box1, cam1_box2, cam1_box3, ...], [cam2_box1, cam2_box2, cam2_box3, ...], [cam3_box1, cam3_box2, cam3_box3, ....]]
    len_bboxes = len(bboxes_list[0])
    #(width, height) = (img_data['width'], img_data['height'])
    width_height_list = [(img_data['width'], img_data['height']) for img_data in img_datas]
    # get image dimensions for resizing
    resized_width_height_list = [get_new_img_size(width_height[0], width_height[1],C.im_size) for width_height in width_height_list]

    gta_list = [np.zeros((len_bboxes, 4)) for _ in range(C.num_cam)]

    for i in range(len_bboxes):
        for j in range(C.num_cam) : 
            bbox = bboxes_list[j][i]
            width, height = width_height_list[j]
            resized_width, resized_height = resized_width_height_list[j]
            # get the GT box coordinates, and resize to account for image resizing
            # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)
            gta_list[j][i, 0] = int(round(bbox['x1'] * (resized_width / float(width))/C.rpn_stride))
            gta_list[j][i, 1] = int(round(bbox['x2'] * (resized_width / float(width))/C.rpn_stride))
            gta_list[j][i, 2] = int(round(bbox['y1'] * (resized_height / float(height))/C.rpn_stride))
            gta_list[j][i, 3] = int(round(bbox['y2'] * (resized_height / float(height))/C.rpn_stride))

    x_roi_list = [[] for _ in range(C.num_cam)]
    y_class_num = []
    y_class_regr_label_coords = []
    #y_class_regr_label = [[] for _ in range(C.num_cam)]
    IoUs = [] # for debugging only

    # Grouped_R[0].shape[0]: number of bboxes (=300 from non_max_suppression)
    for ix in range(grouped_R[0].shape[0]):
        x1s, y1s, x2s, y2s = [], [], [], []
        for R in grouped_R :
            (x1, y1, x2, y2) = R[ix, :]
            x1s.append(int(round(x1)))
            y1s.append(int(round(y1)))
            x2s.append(int(round(x2)))
            y2s.append(int(round(y2)))
        best_iou = 0.0
        best_iou_list = np.zeros((1,))
        best_bbox = -1
        # Iterate through all the ground-truth bboxes to calculate the iou
        for bbox_num in range(len_bboxes):
            curr_iou = 0
            valid_cam = 0
            #cur_iou_list = np.array([0, 0, 0], dtype=float)
            cur_iou_list = np.zeros((C.num_cam,), dtype=float)
            for cam_idx, (x1, y1, x2, y2, gta) in enumerate(zip(x1s, y1s, x2s, y2s, gta_list)) : 
                curr_iou_in_one_cam = 1.0
                if(gta[bbox_num, 0]>-1) : 
                    valid_cam += 1
                    curr_iou_in_one_cam = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])
                #if(curr_iou_in_one_cam > 0):
                #    curr_iou += curr_iou_in_one_cam
                cur_iou_list[cam_idx] =  curr_iou_in_one_cam
            curr_iou = np.sum(cur_iou_list) / valid_cam

            # Find out the corresponding ground-truth bbox_num with larget iou
            if curr_iou > best_iou:
                best_iou = curr_iou
                best_iou_list = cur_iou_list
                best_bbox = bbox_num

#        if (best_iou_list < C.classifier_min_overlap).any() :
#                continue
        if (best_iou_list < C.classifier_min_overlap).all() :
                continue
        elif (best_iou_list < C.classifier_max_overlap).any() and (best_iou_list > C.classifier_max_overlap).any():
                continue
        else:
            ws, hs = [], []
            for x1, y1, x2, y2, x_roi in zip(x1s, y1s, x2s, y2s, x_roi_list):
                w = x2 - x1
                h = y2 - y1
                ws.append(w)
                hs.append(h)
                x_roi.append([x1, y1, w, h])
            IoUs.append(best_iou)

            # hard negative example
            if (best_iou_list < C.classifier_max_overlap).any():
            #if (best_iou_list < C.classifier_max_overlap).all():
                cls_name = 'bg'
            elif (C.classifier_max_overlap <= best_iou_list).all():
                cls_name = bboxes_list[0][best_bbox]['class']
                txs, tys, tws, ths = [], [], [], []
                for x1, y1, w, h, gta in zip(x1s, y1s, ws, hs, gta_list) : 
                    cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0
                    cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0

                    cx = x1 + w / 2.0
                    cy = y1 + h / 2.0

                    tx = (cxg - cx) / float(w)
                    ty = (cyg - cy) / float(h)
                    tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))
                    th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))
                    txs.append(tx)
                    tys.append(ty)
                    tws.append(tw)
                    ths.append(th)
            else:
                print('roi = {}'.format(best_iou))
                raise RuntimeError

        class_num = class_mapping[cls_name]
        class_label = len(class_mapping) * [0]
        class_label[class_num] = 1
        y_class_num.append(copy.deepcopy(class_label))
        labels_coords_list = []
        label_pos = 4 * class_num
        sx, sy, sw, sh = C.classifier_regr_std
        coords_list = []
        labels_list = []
        for i in range(C.num_cam) : 
            coords = [0] * 4*(len(class_mapping)-1)
            labels = [0] * 4*(len(class_mapping)-1)
            if cls_name != 'bg':
                tx, ty, tw, th = txs[i], tys[i], tws[i], ths[i]
                coords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]
                labels[label_pos:4+label_pos] = [1, 1, 1, 1]
            coords_list.append(coords)
            labels_list.append(labels)
        coords_list = np.concatenate(coords_list)
        labels_list = np.concatenate(labels_list)
        labels_coords_in_all_cam = np.concatenate((labels_list, coords_list))
        #    labels_coords = np.concatenate((labels, coords))
        #    labels_coords_list.append(labels_coords)
        #labels_coords_in_all_cam = np.concatenate(labels_coords_list)
        y_class_regr_label_coords.append(labels_coords_in_all_cam)

    if len(x_roi_list[0]) == 0:
        return [None]*C.num_cam, None, None, None

    # one hot code for bboxes from above => x_roi (X)
    Y1 = np.array(y_class_num)
    Y1 = np.expand_dims(Y1, axis=0)
    Y2 = np.array(y_class_regr_label_coords)
    Y2 = np.expand_dims(Y2, axis=0)
    X_list = []
    for x_roi in zip(x_roi_list):
        # bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes
        X = np.array(x_roi)
        X_list.append(X)
    return X_list, Y1, Y2, IoUs

def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):
    """Convert rpn layer to roi bboxes

    Args: (num_anchors = 9)
        rpn_layer: output layer for rpn classification 
            shape (1, feature_map.height, feature_map.width, num_anchors)
            Might be (1, 18, 25, 18) if resized image is 400 width and 300
        regr_layer: output layer for rpn regression
            shape (1, feature_map.height, feature_map.width, num_anchors)
            Might be (1, 18, 25, 72) if resized image is 400 width and 300
        C: config
        use_regr: Wether to use bboxes regression in rpn
        max_boxes: max bboxes number for non-max-suppression (NMS)
        overlap_thresh: If iou in NMS is larger than this threshold, drop the box

    Returns:
        result: boxes from non-max-suppression (shape=(300, 4))
            boxes: coordinates for bboxes (on the feature map)
    """
    regr_layer = regr_layer / C.std_scaling

    anchor_sizes = C.anchor_box_scales   # (3 in here)
    anchor_ratios = C.anchor_box_ratios  # (3 in here)

    assert rpn_layer.shape[0] == 1

    (rows, cols) = rpn_layer.shape[1:3]

    curr_layer = 0

    # A.shape = (4, feature_map.height, feature_map.width, num_anchors) 
    # Might be (4, 18, 25, 18) if resized image is 400 width and 300
    # A is the coordinates for 9 anchors for every point in the feature map 
    # => all 18x25x9=4050 anchors cooridnates
    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))

    for anchor_size in anchor_sizes:
        for anchor_ratio in anchor_ratios:
            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor
            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor
            anchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride
            anchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride
            
            # curr_layer: 0~8 (9 anchors)
            # the Kth anchor of all position in the feature map (9th in total)
            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)
            regr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)

            # Create 18x25 mesh grid
            # For every point in x, there are all the y points and vice versa
            # X.shape = (18, 25)
            # Y.shape = (18, 25)
            X, Y = np.meshgrid(np.arange(cols),np. arange(rows))

            # Calculate anchor position and size for each feature map point
            A[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate
            A[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate
            A[2, :, :, curr_layer] = anchor_x       # width of current anchor
            A[3, :, :, curr_layer] = anchor_y       # height of current anchor

            # Apply regression to x, y, w and h if there is rpn regression layer
            if use_regr:
                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)

            # Avoid width and height exceeding 1
            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])
            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])

            # Convert (x, y , w, h) to (x1, y1, x2, y2)
            # x1, y1 is top left coordinate
            # x2, y2 is bottom right coordinate
            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]
            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]

            # Avoid bboxes drawn outside the feature map
            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])
            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])
            A[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])
            A[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])

            curr_layer += 1

    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)
    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)

    # Apply non_max_suppression
    # Only extract the bboxes. Don't need rpn probs in the later process
    #result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]
    nms_idx_1d, nms_boxes, nms_probs = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)

    nms_idx_A, nms_idx_H, nms_idx_W = np.unravel_index(nms_idx_1d, (rpn_layer.shape[3], rpn_layer.shape[1], rpn_layer.shape[2]))
    nms_idx_2d = np.column_stack((nms_idx_H, nms_idx_W, nms_idx_A))
    return A, nms_idx_2d, nms_boxes, nms_probs

def get_center(box):
    x1, y1, x2, y2 = box
    cx = (x1+x2)/2
    cy = (y1+y2)/2
    return (cx, cy)

def get_epipolar_param(pnt, F, F_to_grid_ratio):
    pnt_reshape = np.array(pnt).reshape(1,1,2)
    line2 = cv2.computeCorrespondEpilines(pnt_reshape*F_to_grid_ratio, 1, F)
    line2 = line2.reshape((-1, 3))[0]
    slope = -line2[0]/line2[1]
    intercept = -line2[2]/line2[1]
    new_intercept = intercept/F_to_grid_ratio
    #print('slope, intercept, new_intercept', slope, intercept, new_intercept)
    return slope, new_intercept

def get_epipolar_line(src, dst, src_pnt, F, grid_rows, grid_cols, F_to_grid_ratio) :
    cur_F = F[src,dst]
    slope, intercept = get_epipolar_param(src_pnt, cur_F, F_to_grid_ratio)
    line_list = []
    x1 = y1 = 0.5
    while x1 < grid_cols and y1 < grid_rows:
        y1 = slope*x1 + intercept
        if(y1 >= 0) : line_list.append((x1, y1))
        #x1 += 1
        x1 += math.sqrt(1/(1+slope**2))
    return line_list 

def get_epipolar_pnt(src1, src2, src1_pnt, src2_pnt, dst, F, F_to_grid_ratio):
    F1 = F[src1][dst]
    F2 = F[src2][dst]
    slope1, intercept1 = get_epipolar_param(src1_pnt, F1, F_to_grid_ratio)
    slope2, intercept2 = get_epipolar_param(src2_pnt, F2, F_to_grid_ratio)
    if(slope1 == slope2) : return [-1, -1]
    x = (intercept2-intercept1) / (slope1-slope2)
    y = slope1*x + intercept1
    return [x, y]

def get_xy_idx(pnt) :
    x = int(round(pnt[0]))
    y = int(round(pnt[1]))
    return x, y 

def calc_dist_pnt_pnts(pnt, pnts) :
    return (pnt[0] - pnts[:, 0])**2 + (pnt[1] - pnts[:, 1])**2

'''
def get_closest_box(cur_cam_pnt, grid_h, grid_w, all_boxes): 
    #all_box = #(4, grid_H, grid_W, anchor=9)
    xi, yi = get_xy_idx(cur_cam_pnt)
    if(xi < 0 or yi < 0 or yi >= grid_h or xi >= grid_w):
        return np.full((4, ), -1)
    cand_boxes = np.reshape(all_boxes, (4, -1)) #(4, 37*66*9)
    cand_boxes = np.transpose(cand_boxes, (1, 0)) #(37*66*9, 4)
    x1, y1, x2, y2 = cand_boxes[:, 0], cand_boxes[:, 1], cand_boxes[:, 2], cand_boxes[:, 3]
    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))
    cand_boxes = np.delete(cand_boxes, idxs, 0)
    if not cand_boxes.any() : 
        return np.full((4, ), -1)
    x1, y1, x2, y2 = cand_boxes[:, 0], cand_boxes[:, 1], cand_boxes[:, 2], cand_boxes[:, 3]
    cand_centers = np.zeros((len(cand_boxes), 2))
    cand_centers[:, 0], cand_centers[:, 1] = (x1+x2)/2, (y1+y2)/2
    cand_dist = calc_dist_pnt_pnts(cur_cam_pnt, cand_centers)
    closest_box = cand_boxes[np.argmin(cand_dist)]
    return closest_box

'''
def get_closest_box(cur_cam_pnt, grid_h, grid_w, all_boxes): 
    #all_box = #(4, grid_H, grid_W, anchor=9)
    xi, yi = get_xy_idx(cur_cam_pnt)
    if(xi < 0 or yi < 0 or yi >= grid_h or xi >= grid_w):
        return np.full((4, ), -1)
    cand_boxes = all_boxes[:, yi, xi, :] #(4, 9)
    cand_boxes = np.transpose(cand_boxes, (1, 0)) #(9, 4)
    x1, y1, x2, y2 = cand_boxes[:, 0], cand_boxes[:, 1], cand_boxes[:, 2], cand_boxes[:, 3]
    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))
    cand_boxes = np.delete(cand_boxes, idxs, 0)
    if not cand_boxes.any() : 
        return np.full((4, ), -1)
    x1, y1, x2, y2 = cand_boxes[:, 0], cand_boxes[:, 1], cand_boxes[:, 2], cand_boxes[:, 3]
    cand_centers = np.zeros((len(cand_boxes), 2))
    cand_centers[:, 0], cand_centers[:, 1] = (x1+x2)/2, (y1+y2)/2
    cand_dist = calc_dist_pnt_pnts(cur_cam_pnt, cand_centers)
    closest_box = cand_boxes[np.argmin(cand_dist)]
    return closest_box

def epipolar(R_list, C, debug_img, iter_num) :
    """Generate matched boxes based on epipolar having top nms pob in one camera.
    Args: (num_anchors = 9)
        R_list: [(all_box, nms_box, nms_prob)]*num_cam,
                #all_box = #(4, grid_H, grid_W, anchor=9)
                #nms_box = #(300, 4)
                #nms_pob = #(300,)
        C: config
        debug_img: [debug_img]*num_cam

    Returns:
        result: [matched_box_list_in_cam0,  matched_box_list_in_cam1, ...]
            matched_box_list_in_cam0 : #(n, 4), n is the number of boxes
    """

    #0. sorting
    all_boxes = []
    prob_cam_box = np.empty((0, 6), float)
    for i, R in enumerate(R_list) :
        all_box, nms_box, nms_prob = R
        all_boxes.append(all_box)
        nms_prob = np.expand_dims(nms_prob, 1)
        cam = np.full((len(nms_box), 1), float(i)) 
        result = np.concatenate((nms_prob, cam, nms_box), axis = 1)
        prob_cam_box = np.append(prob_cam_box, result, axis=0) 
    sort_result =  prob_cam_box[(-prob_cam_box[:, 0]).argsort()]
        
    result = [[] for i in range(C.num_cam)]
    for cbp in sort_result : 
        #print(result[0], '\n', result[1], '\n', result[2])
        if(len(result[0]) > 300) : break
        cur_cam = int(cbp[1])
        box = cbp[2:]
        cur_cam_center = get_center(box)
        #1. get epipolar line in first cam.
        epipolar_pnts = []
        line_cam = -1
        for i in range(C.num_cam):
            if(i == cur_cam):
                continue
            epipolar_pnts = get_epipolar_line(cur_cam, i, cur_cam_center, C.F, C.grid_rows, C.grid_cols, C.F_to_grid_ratio)
            line_cam = i 
            break
        
        for pnt in epipolar_pnts :
            #2. push paired box.
            result[cur_cam].append(box.tolist())
            line_cam_box = get_closest_box(pnt, C.grid_rows, C.grid_cols, all_boxes[line_cam])
            result[line_cam].append(line_cam_box)
            '''
            if(iter_num %10 == 0):
                color = (0, 255, 0)
                src_img = debug_img[cur_cam].copy()
                dst_img = debug_img[line_cam].copy()

                magnified_cur_cam_center =( int(cur_cam_center[0]*C.rpn_stride), int(cur_cam_center[1]*C.rpn_stride))
                magnified_pnt =( int(pnt[0]*C.rpn_stride), int(pnt[1]*C.rpn_stride))
                first_640 =( int(cur_cam_center[0]*C.F_to_grid_ratio), int(cur_cam_center[1]*C.F_to_grid_ratio))
                second_640 =( int(pnt[0]*C.F_to_grid_ratio), int(pnt[1]*C.F_to_grid_ratio))
                first_x1, first_y1, first_x2, first_y2 = box.astype(int)*C.rpn_stride
                second_x1, second_y1, second_x2, second_y2 = line_cam_box.astype(int)*C.rpn_stride
                cv2.rectangle(src_img, (first_x1, first_y1), (first_x2, first_y2), color, 2)
                cv2.rectangle(dst_img, (second_x1, second_y1), (second_x2, second_y2), color, 2)
                cv2.circle(src_img, magnified_cur_cam_center, 3, color, -1)
                cv2.circle(dst_img, magnified_pnt, 3, color, -1)
                print('first', cur_cam, first_640)
                print('second', line_cam, second_640)
                src_img = cv2.resize(src_img, None, fx = .5, fy = .5)
                dst_img = cv2.resize(dst_img, None, fx = .5, fy = .5)
                cv2.imshow('first', src_img)
                cv2.imshow('second', dst_img)
                cv2.waitKey()
            '''
             #3. find matched box in other cams and push them.
            for i in range(C.num_cam):
                if(i == cur_cam or i == line_cam): continue 
                cur_cam_pnt = get_epipolar_pnt(cur_cam, line_cam, cur_cam_center, pnt, i, C.F, C.F_to_grid_ratio) 
                cur_cam_box = get_closest_box(cur_cam_pnt, C.grid_rows, C.grid_cols, all_boxes[i])
                result[i].append(cur_cam_box)

                print('iter_num', iter_num)
                #if(iter_num>0 and iter_num %10 == 0):
                if(iter_num %10 == 0):
                    color = (0, 255, 0)
                    src_img = debug_img[cur_cam].copy()
                    dst_img = debug_img[line_cam].copy()
                    dst_img2 = debug_img[i].copy()

                    magnified_cur_cam_center =( int(cur_cam_center[0]*C.rpn_stride), int(cur_cam_center[1]*C.rpn_stride))
                    magnified_pnt =( int(pnt[0]*C.rpn_stride), int(pnt[1]*C.rpn_stride))
                    magnified_cur_cam_pnt =( int(cur_cam_pnt[0]*C.rpn_stride), int(cur_cam_pnt[1]*C.rpn_stride))
                    first_640 =( int(cur_cam_center[0]*C.F_to_grid_ratio), int(cur_cam_center[1]*C.F_to_grid_ratio))
                    second_640 =( int(pnt[0]*C.F_to_grid_ratio), int(pnt[1]*C.F_to_grid_ratio))
                    third_640 =( int(cur_cam_pnt[0]*C.F_to_grid_ratio), int(cur_cam_pnt[1]*C.F_to_grid_ratio))
                    first_x1, first_y1, first_x2, first_y2 = box.astype(int)*C.rpn_stride
                    second_x1, second_y1, second_x2, second_y2 = line_cam_box.astype(int)*C.rpn_stride
                    third_x1, third_y1, third_x2, third_y2 = cur_cam_box.astype(int)*C.rpn_stride
                    cv2.rectangle(src_img, (first_x1, first_y1), (first_x2, first_y2), color, 2)
                    cv2.rectangle(dst_img, (second_x1, second_y1), (second_x2, second_y2), color, 2)
                    cv2.rectangle(dst_img2, (third_x1, third_y1), (third_x2, third_y2), color, 2)
                    cv2.circle(src_img, magnified_cur_cam_center, 3, color, -1)
                    cv2.circle(dst_img, magnified_pnt, 3, color, -1)
                    cv2.circle(dst_img2, magnified_cur_cam_pnt, 3, color, -1)
                    print('first', cur_cam, first_640)
                    print('second', line_cam, second_640)
                    print('third', i, third_640)
                    src_img = cv2.resize(src_img, None, fx = .5, fy = .5)
                    dst_img = cv2.resize(dst_img, None, fx = .5, fy = .5)
                    dst_img2 = cv2.resize(dst_img2, None, fx = .5, fy = .5)
                    cv2.imshow('first', src_img)
                    cv2.imshow('second', dst_img)
                    cv2.imshow('third', dst_img2)
                    cv2.waitKey()
    result = [np.array(r) for r in result]
    return result

def get_anchor_idx(R, bboxes, width, height, im_size, rpn_stride, vi_max_overlap):
    """get anchor idx which has biggest iou with GT
    Args:
        R: bboxes
    """
    # get image dimensions for resizing
    (resized_width, resized_height) = get_new_img_size(width, height, im_size)

    gta = np.zeros((len(bboxes), 4))
    for bbox_num, bbox in enumerate(bboxes):
        # get the GT box coordinates, and resize to account for image resizing
        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)
        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/rpn_stride))
        gta[bbox_num, 1] = int(round(bbox['y1'] * (resized_height / float(height))/rpn_stride))
        gta[bbox_num, 2] = int(round(bbox['x2'] * (resized_width / float(width))/rpn_stride))
        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/rpn_stride))

    IoUs = [] # for debugging only
    anchor_idx = []

    # R.shape[0]: number of bboxes (=300 from non_max_suppression)
    # Iterate through all the ground-truth bboxes to calculate the iou
    for cur_gta in gta:
        best_iou = 0.0
        best_bbox = -1
        for hi in range(R.shape[1]):
            for wi in range(R.shape[2]):
                for ai in range(R.shape[3]):
                    cur_R = np.rint(R[:, hi, wi, ai])
                    curr_iou = iou(cur_gta, cur_R)
                    if curr_iou > best_iou:
                        best_iou = curr_iou
                        best_bbox_idx = [hi, wi, ai]
                        best_bbox = cur_R

        if vi_max_overlap <= best_iou:
            # this is anchor ! 
            anchor_idx.append(best_bbox_idx)
        else : 
            anchor_idx.append([-1]*3)
        iou(cur_gta, best_bbox)
        IoUs.append(best_iou)
    return anchor_idx, IoUs

def get_anchor_idx_nms(nms_HWA_idx, nms_bboxes, gt_bboxes, width, height, im_size, rpn_stride, vi_max_overlap) :
    """get anchor idx which has biggest iou with GT
    Args:
        nms_HWA_idx : HWA idx of nms (shape : 300, 3)
        nms_bboxes : box of nms (shape : 300, 4)
        gt_bboxes : gt box
    Return :
        anchor_HWA_idx : HWA idx of anchor wich has biggest iou with GT box. If  (shape : 300, 3)
    """
    # get image dimensions for resizing
    (resized_width, resized_height) = get_new_img_size(width, height, im_size)
    gta = np.zeros((len(gt_bboxes), 4))
    for bbox_num, bbox in enumerate(gt_bboxes):
        # get the GT box coordinates, and resize to account for image resizing
        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)
        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/rpn_stride))
        gta[bbox_num, 1] = int(round(bbox['y1'] * (resized_height / float(height))/rpn_stride))
        gta[bbox_num, 2] = int(round(bbox['x2'] * (resized_width / float(width))/rpn_stride))
        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/rpn_stride))

    IoUs = [] # for debugging only
    anchor_idx = []
    # R.shape[0]: number of bboxes (=300 from non_max_suppression)
    # Iterate through all the ground-truth bboxes to calculate the iou
    nms_bboxes = np.rint(nms_bboxes)
    for cur_gta in gta:
        best_iou = 0.0
        best_bbox = -1
        for cur_R, cur_HWA_idx in zip(nms_bboxes, nms_HWA_idx) : 
            curr_iou = iou(cur_gta, cur_R)
            if curr_iou > best_iou:
                best_iou = curr_iou
                best_HWA_idx = cur_HWA_idx 
        if vi_max_overlap <= best_iou:
            # this is anchor ! 
            anchor_idx.append(best_HWA_idx)
        else : 
            anchor_idx.append([-1]*3)
        IoUs.append(best_iou)
    return anchor_idx, IoUs

def get_nms_vi_features(vi_features, nms_idx_list) :
    vi_feature_list = [vi_features[i][tuple(nms_idx.T)] for i, nms_idx in enumerate(nms_idx_list)]
    vi_feature_list = np.stack(vi_feature_list, 0) #(numCam, 300, vi_feature_size)
    return vi_feature_list

def calc_dist(feat1, feat2):
    '''
    calc dist for last axis
    Args :
        feat1 and feat2 have same shape (ex : (2, 3, 4))
    Return :
        dist for last axis (ex : (2, 3))
    '''
    return np.sqrt(np.sum(np.square(feat1 - feat2), -1)) #(m, k)

def get_min_dist_idx(feat, feats, thresh_dist = np.zeros(0)):
    '''
    Args :
        feat (shape : m, n)
        feats (shape : m, k, n)
        thresh_dist : throw away too small dist (shape : m, )
    Return :
        min_dist_idx (shape : m, 1)
    '''
    feat = feat[:, np.newaxis, :] #(m, 1, n)
    dist = calc_dist(feat, feats) #(m, k)
    if(thresh_dist.size) : 
        thresh_dist = thresh_dist[:, np.newaxis] #(m, 1)
        dist[dist<=thresh_dist] = np.inf 
    min_dis_idx = np.argmin(dist, 1) #(m, 1)
    return min_dis_idx

def get_anchor_pos_neg_idx_nms(R_list, img_datas, vi_features, C):
    """get anchor idx, postive idx, negative idx
        Args : 
            R_list : list of R (len : num_cam)
                R: (all_boxes, nms_idx, nms_bboxes, nms_probs) (shape=(H, W, A, 4), shape=(300, 3), shape=(300,4), shape=(300,) )
            img_datas : GT box
            vi_features : view_invariant feature (shape: 1, num_cam, H, W, A, vi_featue_size)
        Output :
            anchor_target_idx : anchor_idx(cam1) and target_idx (shape: 1, 2, num_sample, 4, 1)
                first : num_batch
                second : 0 = anchor, 1 = target
                third : num_sample = camPerm * numGTbox
                fourth : (cam_idx, h_idx, w_idx, a_idx)
    """
    im_size, rpn_stride, vi_max_overlap = C.im_size, C.rpn_stride, C.vi_max_overlap
    width, height = img_datas[0]['width'], img_datas[0]['height']
    num_anchor = len(img_datas[0]['bboxes'])

    #anchor
    anchor_idx_list = -np.ones((C.num_cam, num_anchor, 4), dtype=int)
    nms_HWA_idx_list = np.zeros((C.num_cam, C.num_nms, 3), dtype=int) # HWA
    ious = []
    for cam_idx in range(C.num_cam) :
        gt_bboxes = img_datas[cam_idx]['bboxes']
        _, nms_HWA_idx, nms_bboxes, _ = R_list[cam_idx]
        anchor_idx, iou = get_anchor_idx_nms(nms_HWA_idx, nms_bboxes, gt_bboxes, width, height, im_size, rpn_stride, vi_max_overlap)
        cam_idx_array = np.repeat(cam_idx, num_anchor).reshape(-1, 1)
        anchor_idx_list[cam_idx] = np.concatenate([cam_idx_array, anchor_idx], 1) #(num_anchor, 4)
        nms_HWA_idx_list[cam_idx] = nms_HWA_idx #(num_nms, 3)
        ious.append(iou)

    # delete anchor if even one cam doesn't have that anchor.
    invalid_anchor = np.where(anchor_idx_list[:,:,1:] == [-1, -1, -1])[1] #(axis 1)
    anchor_idx_list = np.delete(anchor_idx_list, invalid_anchor, axis = 1) 
    if not anchor_idx_list.size : return np.zeros(0)

    #add pos
    cam_idx_list = np.arange(C.num_cam)
    cam_idx_perms = np.array(list(permutations(cam_idx_list, 2))) #(Perm_size, 2)
    anchor_pos_idx_list = anchor_idx_list[cam_idx_perms] #(Per_size, 2, 1, 4)
    anchor_pos_idx_list = np.transpose(np.squeeze(anchor_pos_idx_list), (1, 0, 2)) #(2, Per_size, 4)

    #add neg
    vi_features = vi_features[0] #(num_cam, H, W, A, vi_featue_size)

    anchor_idx, positive_idx = anchor_pos_idx_list
    anchor_CHWA_idx = tuple(anchor_idx.T)
    positive_CHWA_idx = tuple(positive_idx.T)
    anchor_feature = vi_features[anchor_CHWA_idx] #(numSample, feature_size)

    postive_feature = vi_features[positive_CHWA_idx] #(numSample, feature_size)
    pos_dist = calc_dist(anchor_feature, postive_feature) #(numSample, )

    negative_cam_idx = positive_CHWA_idx[0]
    nms_vi_features = get_nms_vi_features(vi_features, nms_HWA_idx_list)
    negative_cand_feature = nms_vi_features[negative_cam_idx] #(numSample, 300, vi_feature_size)
    negative_nms_idx = get_min_dist_idx(anchor_feature, negative_cand_feature, pos_dist)
    negative_HWA_idx = nms_HWA_idx_list[negative_cam_idx, negative_nms_idx] #(num_sample, 3)
    negative_CHWA_idx = np.concatenate([negative_cam_idx.reshape(-1, 1), negative_HWA_idx], 1) #(num_sample, 4)

    anchor_pos_neg_idx = np.concatenate([anchor_pos_idx_list, np.expand_dims(negative_CHWA_idx, 0)], 0)#(3, num_sample, 4)
    return anchor_pos_neg_idx[np.newaxis, :, :, :, np.newaxis, np.newaxis]

def get_anchor_pos_neg_idx(R_list, img_datas, vi_features, C):
    """get anchor idx, postive idx, negative idx
        Args : 
            R_list : list of R (len : num_cam)
                R : [all_boxes, _, _, _]
                    all_boxes : roi of P_rpn (shape: H, W, A, 4)
            img_datas : GT box
            vi_features : view_invariant feature (shape: H, W, A, 4)
        Output :
            anchor_target_idx : anchor_idx(cam1) and target_idx (shape: 1, 2, num_sample, 4, 1)
                first : num_batch
                second : 0 = anchor, 1 = target
                third : num_sample = camPerm * numGTbox
                fourth : (cam_idx, h_idx, w_idx, a_idx)
    """
    im_size, rpn_stride, vi_max_overlap = C.im_size, C.rpn_stride, C.vi_max_overlap
    width, height = img_datas[0]['width'], img_datas[0]['height']
    num_anchor = len(img_datas[0]['bboxes'])

    #anchor
    anchor_idx_list = -np.ones((C.num_cam, num_anchor, 4), dtype=int)
    ious = []
    for cam_idx in range(C.num_cam) :
        bboxes = img_datas[cam_idx]['bboxes']
        R, _, _, _ = R_list[cam_idx]
        anchor_idx, iou = get_anchor_idx(R, bboxes, width, height, im_size, rpn_stride, vi_max_overlap)
        cam_idx_array = np.repeat(cam_idx, num_anchor).reshape(-1, 1)
        anchor_idx_list[cam_idx] = np.concatenate([cam_idx_array, anchor_idx], 1)
        ious.append(iou)

    invalid_anchor = np.where(anchor_idx_list[:,:,1:] == [-1, -1, -1])[1] #(axis 1)
    anchor_idx_list = np.delete(anchor_idx_list, invalid_anchor, axis = 1)
    if not anchor_idx_list.size : return np.zeros(0)

    #add pos
    cam_idx_list = np.arange(C.num_cam)
    cam_idx_perms = np.array(list(permutations(cam_idx_list, 2))) #(Perm_size, 2)
    anchor_pos_idx_list = anchor_idx_list[cam_idx_perms] #(Per_size, 2, 1, 4)
    anchor_pos_idx_list = np.transpose(np.squeeze(anchor_pos_idx_list), (1, 0, 2)) #(2, Per_size, 4)

    #add neg
    vi_features = vi_features[0]
    num_sample = anchor_pos_idx_list.shape[1]
    anchor_idx, positive_idx = anchor_pos_idx_list[0], anchor_pos_idx_list[1]
    anchor_idx = (anchor_idx[:, 0], anchor_idx[:, 1], anchor_idx[:, 2], anchor_idx[:, 3])
    positive_idx = (positive_idx[:, 0], positive_idx[:, 1], positive_idx[:, 2], positive_idx[:, 3])
    negative_cam_idx = positive_idx[0]
    anchor_feature = vi_features[anchor_idx] #(numSample, feature_size)
    postive_feature = vi_features[positive_idx] #(numSample, feature_size)
    negative_cand_feature = vi_features[negative_cam_idx] #(numSample, H, W, A, vi_feature_size)

    pos_dist = np.sqrt(np.sum(np.square(anchor_feature - postive_feature), -1)) #(numSample, )
    anchor_feature = anchor_feature[:, np.newaxis, np.newaxis, np.newaxis, :] #(numSample, 1, 1, 1, feature_size)
    neg_cand_dist = np.sqrt(np.sum(np.square(anchor_feature - negative_cand_feature), -1)) #(numSample, H, W, A)
    neg_cand_dist[neg_cand_dist<=pos_dist[:, np.newaxis, np.newaxis, np.newaxis]] = np.inf #throw away hard neg
    neg_cand_dist_2d = neg_cand_dist.reshape((num_sample, -1)) #(numSample, H*W*A)
    negative_idx_1d = np.argmin(neg_cand_dist_2d, 1)
    negative_idx_3d = np.unravel_index(negative_idx_1d, neg_cand_dist.shape[1:])
    negative_idx_4d = [negative_cam_idx] + list(negative_idx_3d)
    negative_idx = np.column_stack(negative_idx_4d)

    '''
    sample_idx = np.arange(num_sample)
    min_dist = neg_cand_dist_2d[(sample_idx, negative_idx_1d)]
    invalid_idx = np.where(min_dist > pos_dist+alpha)[0] #throw away easy neg
    '''

    anchor_pos_neg_idx = np.concatenate([anchor_pos_idx_list, negative_idx[np.newaxis, :, :]])
    #anchor_pos_neg_idx = np.delete(anchor_pos_neg_idx, invalid_idx, 1)
    return anchor_pos_neg_idx[np.newaxis, :, :, :, np.newaxis, np.newaxis]

def reid(vi_feats, R_list, C) :
    """Generate matched boxes based on epipolar having top nms pob in one camera.
    Args: (num_anchors = 9)
        R_list: [(all_box, nms_idx, nms_box, nms_prob)]*num_cam,
                #all_box = #(4, grid_H, grid_W, anchor=9)
                #nms_idx = #(300, 3(=H,W,A)) 
                #nms_box = #(300, 4)
                #nms_pob = #(300,)
        C: config
        debug_img: [debug_img]*num_cam

    Returns:
        result: [matched_box_list_in_cam0,  matched_box_list_in_cam1, ...]
            matched_box_list_in_cam0 : #(n, 4), n is the number of boxes
    """
    all_boxes_list = [np.expand_dims(R[0].transpose(1, 2, 3, 0), 0) for R in R_list]
    nms_idx_list = [R[1] for R in R_list]
    nms_box_list = [R[2] for R in R_list]
    nms_prob_list = [R[3] for R in R_list]
    
    cam_idx_list = np.arange(C.num_cam)
    cam_idx = np.repeat(cam_idx_list, C.num_nms).reshape(-1, 1)
    all_boxes = np.concatenate(all_boxes_list, 0) 
    nms_idx = np.concatenate(nms_idx_list, 0) 
    nms_box = np.concatenate(nms_box_list, 0)
    nms_prob = np.concatenate(nms_prob_list, 0)
    result = np.concatenate([cam_idx, nms_idx, nms_box], 1)

    #sorting
    top_N_result =  result[nms_prob.argsort()[-C.num_nms:]]
    top_N_result = top_N_result.astype(int)
        
    #anchor
    vi_feats = vi_feats[0] #(num_cam, H, W, A, feat_size)
    anchor_cam_idx = top_N_result[:, 0]
    anchor_idx = top_N_result[:, 1:4]
    anchor_box = top_N_result[:, 4:8]
    anchor_H_idx, anchor_W_idx, anchor_A_idx = anchor_idx[:, 0], anchor_idx[:, 1], anchor_idx[:, 2]
    anchor_feats = vi_feats[anchor_cam_idx, anchor_H_idx, anchor_W_idx, anchor_A_idx] #(num_nms, feat_size)
    anchor_feats = anchor_feats[:, np.newaxis, np.newaxis, np.newaxis, :] #(num_nms, 1, 1, 1, feat_size)

    reid_box = -np.ones((C.num_cam, C.num_nms, 4))
    nms_idx = np.arange(C.num_nms)
    reid_box[(anchor_cam_idx, nms_idx)] = anchor_box

    #target
    for i in range(1, C.num_cam):
        target_cam_idx = (anchor_cam_idx+i) % C.num_cam
        target_feats_cand = vi_feats[target_cam_idx] #(num_nms, H, W, A, feat_size)
        dist = np.sqrt(np.sum(np.square(target_feats_cand - anchor_feats), -1)) #(num_nms, H, W, A)
        matched_idx_1d = np.argmin(dist.reshape(len(dist), -1), -1) #(num_nms,)
        matched_idx_H, matched_idx_W, matched_idx_A = np.unravel_index(matched_idx_1d, dist.shape[1:])

        matched_idx = (nms_idx, matched_idx_H, matched_idx_W, matched_idx_A)
        matched_dist = dist[matched_idx] #(num_nms, )
        is_match = matched_dist < C.max_match_dist
        matched_nms_idx = nms_idx[is_match]
        target_cam_idx = target_cam_idx[is_match]
        matched_idx_H = matched_idx_H[is_match]
        matched_idx_W = matched_idx_W[is_match]
        matched_idx_A = matched_idx_A[is_match]
        matched_box_idx = (target_cam_idx, matched_idx_H, matched_idx_W, matched_idx_A)
        reid_box[target_cam_idx, matched_nms_idx] = all_boxes[matched_box_idx] 
    reid_box_list = [reid_box_in_one_cam for reid_box_in_one_cam in reid_box]
    return reid_box_list

def reid300(vi_feats, R_list, C) :
    """Generate matched boxes based on epipolar having top nms pob in one camera.
    Args: (num_anchors = 9)
        R_list: [(all_box, nms_idx, nms_box, nms_prob)]*num_cam,
                #all_box = #(4, grid_H, grid_W, anchor=9)
                #nms_idx = #(300, 3(=H,W,A)) 
                #nms_box = #(300, 4)
                #nms_pob = #(300,)
        C: config
        debug_img: [debug_img]*num_cam

    Returns:
        result: [matched_box_list_in_cam0,  matched_box_list_in_cam1, ...]
            matched_box_list_in_cam0 : #(n, 4), n is the number of boxes
    """
    nms_HWA_idx_list = [R[1] for R in R_list]
    nms_box_list = [R[2] for R in R_list]
    nms_prob_list = [R[3] for R in R_list]
    
    vi_feats = vi_feats[0] #(num_cam, H, W, A, feat_size)
    nms_vi_feats_list = [cur_cam_vi_feats[tuple(nms_idx.T)] for cur_cam_vi_feats, nms_idx in zip(vi_feats, nms_HWA_idx_list)]

    cam_idx_list = np.arange(C.num_cam)
    cam_idx = np.repeat(cam_idx_list, C.num_nms).reshape(-1, 1)
    nms_box = np.concatenate(nms_box_list, 0)
    nms_vi_feats = np.concatenate(nms_vi_feats_list, 0) #(num_cam*num_nms, feat_size)
    nms_prob = np.concatenate(nms_prob_list, 0)
    result = np.concatenate([cam_idx, nms_box, nms_vi_feats], 1)

    idx_in_nms = np.tile(np.arange(300), 3).reshape(-1, 1)
    idx_of_cam_nms = np.concatenate([cam_idx, idx_in_nms], 1)
    top_N_idx_of_cam_nms = idx_of_cam_nms[nms_prob.argsort()[-C.num_nms:]] #(300, 2)

    #sorting
    top_N_result =  result[nms_prob.argsort()[-C.num_nms:]] #(num_nms, 8)
        
    #anchor
    anchor_cam_idx = top_N_result[:, 0].astype(int)
    anchor_box = top_N_result[:, 1:5].astype(int)
    anchor_vi_feats = top_N_result[:, 5:] #(num_nms, feat_size)
    
    nms_arange = np.arange(C.num_nms)
    reid_box = -np.ones((C.num_cam, C.num_nms, 4))
    reid_box[(anchor_cam_idx, nms_arange)] = anchor_box
    nms_box_np = np.stack(nms_box_list, 0) #(num_cam, num_nms, 4)
    nms_vi_features = get_nms_vi_features(vi_feats, nms_HWA_idx_list)

    for offset in range(1, C.num_cam):
        target_cam_idx = (anchor_cam_idx+offset) % C.num_cam
        target_feats = nms_vi_features[target_cam_idx] #(num_nms, num_nms, vi_feature_size)
        target_nms_idx = get_min_dist_idx(anchor_vi_feats, target_feats)
        matched_box = nms_box_np[(target_cam_idx, target_nms_idx)] #(300, 4)
        reid_box[(target_cam_idx, nms_arange)] = matched_box 
    reid_box_list = [reid_box_in_one_cam for reid_box_in_one_cam in reid_box]
    return reid_box_list, anchor_cam_idx
 
def draw(X_list, Y_list_1d, image_data_list, debug_img_list, debug_num_pos_list) : 
    Y_list = []
    for i in range(C.num_cam):
        Y_list.append([Y_list_1d[i*2], Y_list_1d[i*2+1]])

    for X, Y, image_data, debug_img, debug_num_pos in zip(X_list, Y_list, image_data_list, debug_img_list, debug_num_pos_list) : 
        print('Original image:', 'height=%d width=%d'%(image_data['height'], image_data['width']))
        print('Resized image:  height=%d width=%d C.im_size=%d'%(X.shape[1], X.shape[2], C.im_size))
        print('Feature map size: height=%d width=%d C.rpn_stride=%d'%(Y[0].shape[1], Y[0].shape[2], C.rpn_stride))
        print(X.shape)
        print(str(len(Y))+" includes 'y_rpn_cls' and 'y_rpn_regr'")
        print('Shape of y_rpn_cls {}'.format(Y[0].shape))
        print('Shape of y_rpn_regr {}'.format(Y[1].shape))
        print(image_data)

        print('Number of positive anchors for this image: %d' % (debug_num_pos))

    img_list = []
    for X, Y, image_data, debug_img, debug_num_pos in zip(X_list, Y_list, image_data_list, debug_img_list, debug_num_pos_list) : 
        if debug_num_pos==0:
            gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['height']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['height'])
            gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['width']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['width'])
            gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)

            img = debug_img.copy()
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            color = (0, 255, 0)
            cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)
            cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)
            cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)
        else:
            cls = Y[0][0]
            pos_cls = np.where(cls==1)
            print('pos_cls', pos_cls)
            regr = Y[1][0]
            pos_regr = np.where(regr==1)
            print('pos_regr', pos_regr)
            print('y_rpn_cls for possible pos anchor: {}'.format(cls[pos_cls[0][0],pos_cls[1][0],:]))
            print('y_rpn_regr for positive anchor: {}'.format(regr[pos_regr[0][0],pos_regr[1][0],:]))

            gt_x1, gt_x2 = image_data['bboxes'][0]['x1']*(X.shape[2]/image_data['width']), image_data['bboxes'][0]['x2']*(X.shape[2]/image_data['width'])
            gt_y1, gt_y2 = image_data['bboxes'][0]['y1']*(X.shape[1]/image_data['height']), image_data['bboxes'][0]['y2']*(X.shape[1]/image_data['height'])
            gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)

            img = debug_img.copy()
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            color = (0, 255, 0)
            #   cv2.putText(img, 'gt bbox', (gt_x1, gt_y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 1)
            cv2.rectangle(img, (gt_x1, gt_y1), (gt_x2, gt_y2), color, 2)
            cv2.circle(img, (int((gt_x1+gt_x2)/2), int((gt_y1+gt_y2)/2)), 3, color, -1)

            # Add text
            textLabel = 'gt bbox'
            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,0.5,1)
            textOrg = (gt_x1, gt_y1+5)
            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)
            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)
            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 0.5, (0, 0, 0), 1)

            # Draw positive anchors according to the y_rpn_regr
            for i in range(debug_num_pos):

                color = (100+i*(155/4), 0, 100+i*(155/4))

                idx = pos_regr[2][i*4]/4
                anchor_size = C.anchor_box_scales[int(idx/3)]
                anchor_ratio = C.anchor_box_ratios[2-int((idx+1)%3)]

                center = (pos_regr[1][i*4]*C.rpn_stride, pos_regr[0][i*4]*C.rpn_stride)
                print('Center position of positive anchor: ', center)
                cv2.circle(img, center, 3, color, -1)
                anc_w, anc_h = anchor_size*anchor_ratio[0], anchor_size*anchor_ratio[1]
                cv2.rectangle(img, (center[0]-int(anc_w/2), center[1]-int(anc_h/2)), (center[0]+int(anc_w/2), center[1]+int(anc_h/2)), color, 2)
#         cv2.putText(img, 'pos anchor bbox '+str(i+1), (center[0]-int(anc_w/2), center[1]-int(anc_h/2)-5), cv2.FONT_HERSHEY_DUPLEX, 0.5, color, 1)
        img_list.append(img)

    print('Green bboxes is ground-truth bbox. Others are positive anchors')
    plt.figure(figsize=(8,8))
    for i, img in enumerate(img_list) : 
        coord = int('1'+str(C.num_cam)+str(i+1))
        plt.grid()
        plt.subplot(coord)
        plt.imshow(img)
    plt.show()

def draw_inst(img, x1, y1, x2, y2, cls, color, prob=None):
    cv2.rectangle(img,(x1, y1), (x2, y2), color, 4)
    textLabel = '{}:{}'.format(cls,int(100*prob)) if prob else cls
    '''
    (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)
    textOrg = (x1, y1-0)
    cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)
    cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)
    cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)
    '''

    (text_w,text_h) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_DUPLEX,1,1)
    textOrg = (x1-10, y1-text_h)
    cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)

    return img
   
def draw_gt_pred(img, dets, gts, ratio, save_path=None):
    det_color = (255, 0, 0)
    gt_color = (0, 255, 0)
    for det in dets :
        x1, y1, x2, y2, prob, cls = det['x1'], det['y1'], det['x2'], det['y2'], det['prob'], det['class']
        (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)
        img = draw_inst(img, real_x1, real_y1, real_x2, real_y2, cls, det_color, prob)
        #print('det', '%s, x1:%d, y1:%d, x2:%d, y2:%d, prob:%f'%(cls, real_x1, real_y1, real_x2, real_y2, prob))

    for gt in gts :
        x1, y1, x2, y2, cls = gt['x1'], gt['y1'], gt['x2'], gt['y2'], gt['class']
        img = draw_inst(img, x1, y1, x2, y2, cls, gt_color)
        #print('gt', '%s, x1:%d, y1:%d, x2:%d, y2:%d, prob:%f'%(cls, x1, y1, x2, y2, 1.0))
    if(save_path) : 
        cv2.imwrite(save_path, img)
    else : 
        cv2.imshow('gt_pred', img)
        cv2.waitKey()

def draw_box(image, box, name, color = (0, 255, 0)):
    x1, y1, x2, y2 = box
    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
    image = cv2.resize(image, (320, 180))
    cv2.imshow(name, image)

def draw_box_from_idx(all_boxes, all_images, idx, rpn_stride, name):
    cam_idx, H_idx, W_idx, A_idx = idx
    image = np.copy(all_images[cam_idx])
    box = all_boxes[(cam_idx, H_idx, W_idx, A_idx)]
    box = box.astype(int)*rpn_stride
    draw_box(image, box, name)
 
def draw_nms(nms_list, debug_img, rpn_stride) : 
    nms_np = np.stack(nms_list, 0) #(num_cam, 300, 4)
    nms_np = nms_np.astype(int)*rpn_stride
    for cam_idx, nms in enumerate(nms_np) :
        img = np.copy(debug_img[cam_idx])
        window_name = 'nms' + str(cam_idx)
        for box in nms:
            draw_box(img, box, window_name)
    cv2.waitKey(0)

def draw_anchor_pos_neg(all_boxes, anchor_pos_neg_idx, debug_img, rpn_stride) : 
    all_boxes = np.stack(all_boxes, 0) #(num_cam, 4, H, W, A)
    all_boxes = all_boxes.transpose(0, 2, 3, 4, 1) #(num_cam, H, W, A, 4)
    anc_idx, pos_idx, neg_idx = anchor_pos_neg_idx[0, :, :, :, 0, 0] #(num_sample, 4)
    for cur_anc_idx, cur_pos_idx, cur_neg_idx in zip(anc_idx, pos_idx, neg_idx) :
        draw_box_from_idx(all_boxes, debug_img, cur_anc_idx, rpn_stride, 'anchor')
        draw_box_from_idx(all_boxes, debug_img, cur_pos_idx, rpn_stride, 'pos')
        draw_box_from_idx(all_boxes, debug_img, cur_neg_idx, rpn_stride, 'neg')
        cv2.waitKey(0)

def draw_reid(grouped_R, anchor_cam_idx, debug_img, rpn_stride) : 
    grouped_boxes = np.stack(grouped_R, 1) #(300, num_cam, 4)
    grouped_boxes = grouped_boxes.astype(int)*rpn_stride
    grouped_boxes = grouped_boxes[::-1]
    anchor_cam_idx = anchor_cam_idx[::-1]
    for (cur_grouped_boxes, anchor_cam) in zip(grouped_boxes, anchor_cam_idx) :
        for cam_idx, box in enumerate(cur_grouped_boxes):
            color = (0, 0, 255) if (cam_idx == anchor_cam) else (0, 255, 0) 
            window_name = 'reid' + str(cam_idx)
            draw_box(np.copy(debug_img[cam_idx]), box, window_name, color)
        cv2.waitKey(0)

def train(train_path, val_path, result_img_path):
    # Augmentation flag
    horizontal_flips = False # Augment with horizontal flips in training. 
    vertical_flips = False   # Augment with vertical flips in training. 
    rot_90 = False           # Augment with 90 degree rotations in training. 

    C.use_horizontal_flips = horizontal_flips
    C.use_vertical_flips = vertical_flips
    C.rot_90 = rot_90

    #--------------------------------------------------------#
    # This step will spend some time to load the data        #
    #--------------------------------------------------------#
    st = time.time()
    train_imgs, classes_count, class_mapping = get_data(train_path, C.num_cam)
    print()
    print('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )

    if 'bg' not in classes_count:
        classes_count['bg'] = 0
        class_mapping['bg'] = len(class_mapping)
# e.g.
#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}
#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}
    C.class_mapping = class_mapping

    print('Training images per class:')
    pprint.pprint(classes_count)
    print('Num classes (including bg) = {}'.format(len(classes_count)))
    print(class_mapping)

# Save the configuration
    with open(C.config_output_filename, 'wb') as config_f:
        pickle.dump(C,config_f)
        print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))


# Shuffle the images with seed
    random.seed(1)
    random.shuffle(train_imgs)

    print('Num train samples (images) {}'.format(len(train_imgs)))

# Get train data generator which generate X, Y, image_data
    #data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')
    data_gen_train = get_anchor_gt_thread(train_imgs, C, get_img_output_length, mode='train')
    '''
    data_gen_train_org = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')

    start = time.time()
    X_list_thread, Y_list_1d_thread, image_data_list_thread, debug_img_list_thread, debug_num_pos_list_thread = next(data_gen_train_thread)    
    end_thread = time.time()
    X_list_org, Y_list_1d_org, image_data_list_org, debug_img_list_org, debug_num_pos_list_org = next(data_gen_train_org)
    end_org = time.time()

    end_thread = time.time()
    print('thread time', end_thread-start)
    print('org time', end_org-end_thread)
    draw(X_list_org, Y_list_1d_org, image_data_list_org, debug_img_list_org, debug_num_pos_list_org)
    draw(X_list_thread, Y_list_1d_thread, image_data_list_thread, debug_img_list_thread, debug_num_pos_list_thread)
    '''
    '''
    X_list, Y_list_1d, image_data_list, debug_img_list, debug_num_pos_list = next(data_gen_train)
    draw(X_list, Y_list_1d, image_data_list, debug_img_list, debug_num_pos_list)
    '''

    input_shape_img = (None, None, 3)

    num_cam = C.num_cam
    img_input = []
    roi_input = []
    for i in range(num_cam) : 
        img_input.append(Input(shape=input_shape_img))
        roi_input.append(Input(shape=(None, 4)))
    #anchor_pos_neg_input = [Input(shape=(None, 2)), Input(shape=(None, 2)), Input(shape=(None, 2))]

# define the base network (VGG here, can be Resnet50, Inception, etc)

    shared_layer = nn_base_model()
    shared_layers = []
    for i in range(num_cam):
        shared_layers.append(shared_layer(img_input[i]))
        
# define the RPN, built on the base layers
    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9
    rpn_body, rpn_class, rpn_regr = rpn_layer_model(num_anchors)
    view_invariant_layer = view_invariant_layer_model(C.grid_rows, int(C.grid_cols), num_anchors, C.view_invar_feature_size)
    rpns = []
    view_invariants = []
    for i in range(num_cam) :
        body = rpn_body(shared_layers[i])
        cls = rpn_class(body)
        regr = rpn_regr(body)
        view_invariant = view_invariant_layer(body)
        rpns.extend([cls, regr])
        view_invariants.append(view_invariant)

    view_invariant_conc = view_invariant_conc_layer(view_invariants)
    classifier = classifier_layer(shared_layers, roi_input, C.num_rois, C.num_features, num_cam, nb_classes=len(classes_count))

    model_rpn = Model(img_input, rpns)
    model_view_invariant = Model(img_input, view_invariant_conc)
    classifier_input = img_input + roi_input
    model_classifier = Model(classifier_input, classifier)

# this is a model that holds both the RPN and the classifier, used to load/save weights for the models
    model_all = Model(classifier_input, rpns + [view_invariant_conc] + classifier)

# Because the google colab can only run the session several hours one time (then you need to connect again), 
# we need to save the model and load the model to continue training
    if not os.path.isfile(C.model_path):
        #If this is the begin of the training, load the pre-traind base network such as vgg-16
        try:
            print('This is the first time of your training')
            print('loading weights from {}'.format(C.base_net_weights))
            model_rpn.load_weights(C.base_net_weights, by_name=True)
            model_view_invariant.load_weights(C.base_net_weights, by_name=True)
            model_classifier.load_weights(C.base_net_weights, by_name=True)
        except:
            print('Could not load pretrained model weights. Weights can be found in the keras application folder \
                https://github.com/fchollet/keras/tree/master/keras/applications')
        
        # Create the record.csv file to record losses, acc and mAP
        #record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])
        record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'loss_vi', 'curr_loss', 'elapsed_time', 'mAP'])
    else:
        # If this is a continued training, load the trained model from before
        print('Continue training based on previous trained model')
        print('Loading weights from {}'.format(C.model_path))
        model_rpn.load_weights(C.model_path, by_name=True)
        model_view_invariant.load_weights(C.model_path, by_name=True)
        model_classifier.load_weights(C.model_path, by_name=True)
        
        # Load the records
        record_df = pd.read_csv(record_path)

        r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']
        r_class_acc = record_df['class_acc']
        r_loss_rpn_cls = record_df['loss_rpn_cls']
        r_loss_rpn_regr = record_df['loss_rpn_regr']
        r_loss_class_cls = record_df['loss_class_cls']
        r_loss_class_regr = record_df['loss_class_regr']
        r_loss_vi = record_df['loss_vi']
        r_curr_loss = record_df['curr_loss']
        r_elapsed_time = record_df['elapsed_time']
        r_mAP = record_df['mAP']

        print('Already train %dK batches'% (len(record_df)))

    optimizer = Adam(lr=1e-5)
    optimizer_view_invariant = Adam(lr=1e-5)
    optimizer_classifier = Adam(lr=1e-5)
    rpn_loss = []
    for i in range(num_cam) : 
        rpn_loss.extend([rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])
    model_rpn.compile(optimizer=optimizer, loss=rpn_loss)
    model_view_invariant.compile(optimizer=optimizer_view_invariant, loss=view_invariant_loss(C.vi_alpha))
    model_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1, num_cam)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})
    model_all.compile(optimizer='sgd', loss='mae')

    def named_logs(model, logs):
      result = {}
      for l in zip(model.metrics_names, logs):
        result[l[0]] = l[1]
      return result

# Training setting
    total_epochs = len(record_df)
    r_epochs = len(record_df)

    epoch_length = 500
    num_epochs = 160
    iter_num = 0

    total_epochs += num_epochs

    losses = np.zeros((epoch_length, 6))
    rpn_accuracy_rpn_monitor = []
    rpn_accuracy_for_epoch = []

    if len(record_df)==0:
        best_loss = np.Inf
        best_mAP = 0
    else:
        best_loss = np.min(r_curr_loss)
        best_mAP = np.max(r_mAP)

    print(len(record_df))

    start_time = time.time()
    for epoch_num in range(num_epochs):

        progbar = generic_utils.Progbar(epoch_length)
        print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))
        
        r_epochs += 1

        while True:
            try:

                if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:
                    mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)
                    rpn_accuracy_rpn_monitor = []
#                 print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))
                    if mean_overlapping_bboxes == 0:
                        print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')

                # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])
                X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)

                # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]
                loss_rpn = model_rpn.train_on_batch(X, Y)
                print(loss_rpn)
                '''
                with open('mv_train_two_XY.pickle', 'wb') as f:
                    pickle.dump([X, Y],f)
                '''
                
                #loss_rpn = [0]*7
                #tensorboard_rpn.on_epoch_end(iter_num, named_logs(model_rpn, loss_rpn))
                # Get predicted rpn from rpn model [rpn_cls, rpn_regr]
                P_rpn = model_rpn.predict_on_batch(X)

                # Convert rpn layer to roi bboxes
                # R_list : list of R, (len=num_cam)
                # R: (all_boxes, nms_idx, nms_bboxes, nms_probs) (shape=(H, W, A, 4), shape=(300, 3), shape=(300,4), shape=(300,) )
                # nms_idx : (H, W, A)
                R_list = []
                for i in range(num_cam):
                    cam_idx = i*2
                    rpn_probs = P_rpn[cam_idx]
                    rpn_boxs = P_rpn[cam_idx+1]
                    R = rpn_to_roi(rpn_probs, rpn_boxs, C, K.common.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes = C.num_nms)
                    R_list.append(R)
                           
                #nms_list = [R[2] for R in R_list]
                #draw_nms(nms_list, debug_img, C.rpn_stride) 

                view_invariant_features = model_view_invariant.predict_on_batch(X)
                #anchor_pos_neg_idx = get_anchor_pos_neg_idx(R_list, img_data, view_invariant_features, C)
                anchor_pos_neg_idx = get_anchor_pos_neg_idx_nms(R_list, img_data, view_invariant_features, C)
                '''
                pred_box = np.array([R[2] for R in R_list])
                pred_box_idx = np.array([R[1] for R in R_list]) #(num_cam, 300, 3)
                cam_idx_arr = np.repeat(np.arange(C.num_cam), C.num_nms).reshape(C.num_cam, C.num_nms, 1)
                pred_box_idx = np.concatenate((cam_idx_arr, pred_box_idx), axis = 2)
                all_box_emb = np.squeeze(view_invariant_features)
                reid_gt_pickle = [pred_box, pred_box_idx, all_box_emb, anchor_pos_neg_idx]
                with open('mv_train_two_reid_gt.pickle', 'wb') as f:
                    pickle.dump(reid_gt_pickle,f)
                '''

                #if(anchor_pos_neg_idx.size == 0):
                #    anchor_pos_neg_idx = get_anchor_pos_neg_idx(R_list, img_data, view_invariant_features, C)

                if(anchor_pos_neg_idx.size == 0):
                    #print('anchor_pos_neg_idx.size == 0')
                    continue    
                #all_boxes = [R[0] for R in R_list]
                #draw_anchor_pos_neg(all_boxes, anchor_pos_neg_idx, debug_img, C.rpn_stride) 
                vi_loss = model_view_invariant.train_on_batch(X, anchor_pos_neg_idx)
                #print('vi_loss', vi_loss)
                
                # grouped_R : [[box1, .... ,box300], [box1, .... ,box300], [box1, .... ,box300]], len(grouped_R) = cam_num, top 300 grouped R where R in each cam is grouped by epipolar geometry. 
                grouped_R, anchor_cam_idx = reid300(view_invariant_features, R_list, C)
                #grouped_R = reid(view_invariant_features, R_list, C)
                '''
                pred_box_emb = all_box_emb[tuple(pred_box_idx.T)].transpose((1, 0, 2))
                pred_box_prob = np.array([R[3] for R in R_list])
                reid_gt = np.array(grouped_R).transpose((1, 0, 2))
                reid_pickle = [pred_box, pred_box_emb, pred_box_prob, reid_gt]
                with open('mv_train_two_reid.pickle', 'wb') as f:
                    pickle.dump(reid_pickle,f)
                '''

                #draw_reid(grouped_R, anchor_cam_idx, debug_img, C.rpn_stride)

                # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format
                # X2: [bboxes1, bboxes2, ...,bboxes_num_cam], bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes
                # Y1: one hot code for bboxes from above => x_roi (X)
                # Y2: corresponding labels and corresponding gt bboxes, #(None, 8 * # of fg classes * num_cam) 
                #X2, Y1, Y2, IouS = calc_iou(R, img_data, C, class_mapping)
                #grouped_R = np.zeros((3, 300, 4))
                X2, Y1, Y2, IouS = calc_iou(grouped_R, img_data, C, class_mapping)
                '''
                pred_boxes = np.array(grouped_R).transpose(1, 0, 2)
                x_roi_gt = np.squeeze(np.array(X2)).transpose(1, 0, 2)
                y_cls_gt = np.squeeze(Y1)
                y_regr_gt = np.squeeze(Y2)
                classifier_pickle = [pred_boxes, x_roi_gt, y_cls_gt, y_regr_gt]
                with open('mv_train_two_classfier_gt.pickle', 'wb') as f:
                    pickle.dump(classifier_pickle,f)
                '''


                # If X2 is None means there are no matching bboxes
                if X2[0] is None:
                    rpn_accuracy_rpn_monitor.append(0)
                    rpn_accuracy_for_epoch.append(0)
                    #print('calc iou == 0')
                    #print('loss_rpn_cls', loss_rpn[1], 'loss_rpn_regr', loss_rpn[2], 'vi_loss', vi_loss)
                    continue
                
                # Find out the positive anchors and negative anchors
                neg_samples = np.where(Y1[0, :, -1] == 1)
                pos_samples = np.where(Y1[0, :, -1] == 0)
                print('pos_samples', pos_samples)

                if len(neg_samples) > 0:
                    neg_samples = neg_samples[0]
                else:
                    neg_samples = []

                if len(pos_samples) > 0:
                    pos_samples = pos_samples[0]
                else:
                    pos_samples = []

                rpn_accuracy_rpn_monitor.append(len(pos_samples))
                rpn_accuracy_for_epoch.append((len(pos_samples)))

                if C.num_rois > 1:
                    # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples
                    if len(pos_samples) > 0 :
                        print(img_data)
                        print('pos_samples > 0')
                    if len(pos_samples) < C.num_rois//2:
                        selected_pos_samples = pos_samples.tolist()
                    else:
                        selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()
                    
                    # Randomly choose (num_rois - num_pos) neg samples
                    try:
                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()
                    except:
                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()
                    
                    # Save all the pos and neg samples in sel_samples
                    sel_samples = selected_pos_samples + selected_neg_samples
                else:
                    # in the extreme case where num_rois = 1, we pick a random pos or neg sample
                    selected_pos_samples = pos_samples.tolist()
                    selected_neg_samples = neg_samples.tolist()
                    if np.random.randint(0, 2):
                        sel_samples = random.choice(neg_samples)
                    else:
                        sel_samples = random.choice(pos_samples)

                # training_data: [X, final_X2]
                # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]
                #  X                     => list of img_data resized image, len(X) = num_cam
                #  final_X2              => list of num_rois (4 in here) bboxes which contains selected neg and pos, len(final_X2) = num_cam
                #  final_Y1 => one hot encode for num_rois bboxes which contains selected neg and pos, #(1, 4, fg+bg)
                #  final_Y2 => labels and gt bboxes for num_rois bboxes which contains selected neg and pos, #(1, 4, fg*num_cam*8)
                final_X2 = [ x2[:, sel_samples, :] for x2 in X2] 
                final_Y1 = Y1[:, sel_samples, :]
                final_Y2 = Y2[:, sel_samples, :]
                loss_class = model_classifier.train_on_batch( X+final_X2, [final_Y1, final_Y2])

                
                print(final_Y1)
                #cls_box, cls_prob = classfier_output_to_box_prob(final_X2, final_Y1, final_Y2, C, 0, False) 
                #draw_cls_box_prob(debug_img, cls_box, cls_prob, C, is_nms=False)
                #vi_loss = 0
                #loss_class = [0]*4

                loss_rpn_cls_all_cam = loss_rpn_regr_all_cam = 0
                for i in range(C.num_cam) : 
                    loss_rpn_cls_all_cam += loss_rpn[2*i+1]
                    loss_rpn_regr_all_cam += loss_rpn[2*(i+1)]
                loss_rpn_cls_all_cam /= C.num_cam 
                loss_rpn_regr_all_cam /= C.num_cam 

                losses[iter_num, 0] = loss_rpn_cls_all_cam
                losses[iter_num, 1] = loss_rpn_regr_all_cam

                losses[iter_num, 2] = loss_class[1]
                losses[iter_num, 3] = loss_class[2]
                losses[iter_num, 4] = loss_class[3]

                losses[iter_num, 5] = vi_loss

                iter_num += 1

                progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),
                                          ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3])), ('vi', np.mean(losses[:iter_num, 5]))])

                if iter_num == epoch_length:
                    loss_rpn_cls = np.mean(losses[:, 0])
                    loss_rpn_regr = np.mean(losses[:, 1])
                    loss_class_cls = np.mean(losses[:, 2])
                    loss_class_regr = np.mean(losses[:, 3])
                    class_acc = np.mean(losses[:, 4])
                    loss_vi = np.mean(losses[:, 5])

                    mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)
                    #mean_overlapping_bboxes = 0
                    rpn_accuracy_for_epoch = []

                    if C.verbose:
                        print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))
                        print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))
                        print('Loss RPN classifier: {}'.format(loss_rpn_cls))
                        print('Loss RPN regression: {}'.format(loss_rpn_regr))
                        print('Loss Detector classifier: {}'.format(loss_class_cls))
                        print('Loss Detector regression: {}'.format(loss_class_regr))
                        print('Loss vi: {}'.format(loss_vi))
                        print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr+loss_vi))
                        print('Elapsed time: {}'.format(time.time() - start_time))
                        elapsed_time = (time.time()-start_time)/60

                    curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr + loss_vi
                    iter_num = 0

                    model_all.save_weights(C.model_path)
                    cur_mAP = calc_map_from_model(val_path, [model_rpn, model_view_invariant, model_classifier], result_img_path)
                    #cur_mAP = calc_map(val_path, result_img_path)
                    #if curr_loss < best_loss:
                    if cur_mAP > best_mAP:
                        if C.verbose:
                            print('Total mAP increased from {} to {}, saving weights'.format(cur_mAP,best_mAP))
                        best_mAP = cur_mAP
                        model_all.save_weights(C.best_model_path)

                    new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), 
                               'class_acc':round(class_acc, 3), 
                               'loss_rpn_cls':round(loss_rpn_cls, 3), 
                               'loss_rpn_regr':round(loss_rpn_regr, 3), 
                               'loss_class_cls':round(loss_class_cls, 3), 
                               'loss_class_regr':round(loss_class_regr, 3), 
                               'loss_vi':round(loss_vi, 3), 
                               'curr_loss':round(curr_loss, 3), 
                               'elapsed_time':round(elapsed_time, 3), 
                               'mAP': round(cur_mAP, 3)}

                    record_df = record_df.append(new_row, ignore_index=True)
                    record_df.to_csv(record_path, index=0)

                    start_time = time.time()
                    break

            except Exception as e:
                print('Exception: {}'.format(e))
                logging.error(traceback.format_exc())
                continue

    print('Training complete, exiting.')

    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')
    plt.title('mean_overlapping_bboxes')
    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')
    plt.title('class_acc')

    plt.show()

    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')
    plt.title('loss_rpn_cls')
    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')
    plt.title('loss_rpn_regr')
    plt.show()


    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')
    plt.title('loss_class_cls')
    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')
    plt.title('loss_class_regr')
    plt.show()

    plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')
    plt.title('total_loss')
    plt.show()

def plot_record(record_df) : 
    r_epochs = len(record_df)

    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['mean_overlapping_bboxes'], 'r')
    plt.title('mean_overlapping_bboxes')

    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['class_acc'], 'r')
    plt.title('class_acc')

    plt.show()

    plt.figure(figsize=(15,5))

    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_cls'], 'r')
    plt.title('loss_rpn_cls')

    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['loss_rpn_regr'], 'r')
    plt.title('loss_rpn_regr')
    plt.show()
    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['loss_class_cls'], 'r')
    plt.title('loss_class_cls')

    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['loss_class_regr'], 'r')
    plt.title('loss_class_regr')
    plt.show()
    plt.figure(figsize=(15,5))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, r_epochs), record_df['curr_loss'], 'r')
    plt.title('total_loss')

    plt.subplot(1,2,2)
    plt.plot(np.arange(0, r_epochs), record_df['elapsed_time'], 'r')
    plt.title('elapsed_time')

    plt.show()

def find_best_model(val_path, all_model_path):
    map_list = []
    epoch = 42
    while(1):
        model_path = all_model_path%(epoch)
        if os.path.isfile(model_path):
            map_list.append(calc_map(val_path, save_dir = '/data3/sap/result', model_path = model_path))
        else :
            break
        epoch += 1

    for i, mAP in enumerate(map_list) :
        print(i, mAP)
    best_map = max(map_list)
    best_epoch = map_list.index(best_map)
    print('best_mAP', best_epoch, best_map)
    best_model_path = all_model_path%(best_epoch)
    os.system('cp %s %s'%(best_model_path, C.best_model_path))
    
    

if __name__ == '__main__' : 
#start 
    base_weight = 'mv_rpn_model.hdf5'
    save_name = 'mv_v2_interpark2'
    train_file = 'mv_train.txt'
    val_file = 'mv_val.txt'
    test_file = 'mv_test.txt'
    base_path = '/data3/sap/frcnn_keras'
    '''
    save_name = 'mv_interpark18'
    train_file = 'mv_interpark18_train.txt'
    val_file = 'mv_interpark18_val.txt'
    test_file = 'mv_interpark18_test.txt'
    base_path = '/data3/sap/frcnn_keras'
    '''

    data_folder = 'data'
    model_folder = 'model'
    record_folder = 'record'
    config_folder = 'config'
    result_img_folder = 'result'

    train_path = '%s/%s/%s'%(base_path, data_folder, train_file) 
    #train_path = '/home/sap/frcnn_keras/data/test_data/interpark.txt'
    val_path = '%s/%s/%s'%(base_path, data_folder, val_file)
    test_path = '%s/%s/%s'%(base_path, data_folder, test_file)
    base_weight_path ='%s/%s/%s'%(base_path, model_folder, base_weight) 
    output_weight_path = '%s/%s/%s_model.hdf5' %(base_path, model_folder, save_name)
    best_model_path = '%s/%s/%s_best_model.hdf5' %(base_path, model_folder, save_name)
    all_model_path = '%s/%s/%s_%s_model.hdf5' %(base_path, model_folder, save_name, '%d')
    config_output_filename = '%s/%s/%s_config.pickle'%(base_path, config_folder, save_name)
    record_path = '%s/%s/%s_record.csv'%(base_path, record_folder, save_name)
    result_img_path = '%s/%s/result-%s'%(base_path, result_img_folder, save_name)
    demo_bbox_threshold = 0.7
    num_demo = 100
    F01 = [[2.459393284555216e-07, 1.240428133324114e-05, -0.0019388276339150634], [1.3911908206709622e-05, -3.0469249778638727e-06, -0.00732105994034854], [-0.0017512954375120127, -0.00015617893705877073, 1.0]]
    F10 = [[2.459398251005644e-07, 1.3911909814174001e-05, -0.001751295584798182], [1.2404286166647384e-05, -3.0469314816431586e-06, -0.00015617819543456424], [-0.0019388285855039822, -0.007321060074713767, 1.0]]
    F02 = [[1.5205480713149612e-06, 9.161841415733138e-06, -0.002173851911725283], [-3.9633584206531314e-07, 6.4197842298352806e-06, 0.0015436659310524847], [-0.0008260013004840316, -0.006607662623582755, 1.0]]
    F20 = [[1.520548069654194e-06, -3.9633583414147577e-07, -0.0008260013007094849], [9.161841403010676e-06, 6.419784245600231e-06, -0.006607662618055121], [-0.002173851908850416, 0.0015436659205338987, 1.0]]
    F12 = [[-7.89652807295317e-07, 7.688891158506741e-06, -0.0010792230317055729], [5.9428192631561134e-06, 8.330558193275425e-06, -0.003611465890521748], [-0.0009379108709930972, -0.0037408066830639797, 1.0]]
    F21 = [[-7.89652807897754e-07, 5.942819265995639e-06, -0.0009379108715031892], [7.688891158958637e-06, 8.33055818670516e-06, -0.003740806681812203], [-0.001079223031796278, -0.003611465890419871, 1.0]]
    F00 = F11 = F22 = [[0.0]*3]*3
    F = np.array([[F00, F01, F02], [F10, F11, F12], [F20, F21, F22]])

    try : 
        with open(config_output_filename, 'rb') as f_in:
            C = pickle.load(f_in)
    except : 
        C = Config()
    C.base_net_weights = base_weight_path
    C.model_path = output_weight_path
    C.best_model_path = best_model_path
    C.config_output_filename = config_output_filename
    C.record_path = record_path
    C.vi_max_overlap = .3
    C.F = F

    train(train_path, val_path, result_img_path)
    #show_demos(test_path, demo_bbox_threshold, num_demo, result_img_path)
    #calc_map(test_path, result_img_path)
    #find_best_model(val_path, all_model_path)


